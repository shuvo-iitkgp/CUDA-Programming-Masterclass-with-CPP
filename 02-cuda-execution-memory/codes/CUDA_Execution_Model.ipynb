{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Computer architecures classification :\n",
        "\n",
        "1. SISD stands for Single Instruction, Single Data. It's a computer architecture where a single processor executes one instruction at a time on a single data stream. This means that instructions are processed sequentially, one after the other, making SISD a sequential processing system.\n",
        "\n",
        "2. SIMD stands for Single Instruction, Multiple Data. It's a parallel processing technique where a single instruction is applied to multiple data elements simultaneously, typically using special wide registers.\n",
        "\n",
        "3. MISD, which stands for Multiple Instruction, Single Data, is a type of parallel computer architecture where multiple processing units operate on the same data stream using different instructions. It's primarily a theoretical concept, and no real-world systems are built using it\n",
        "\n",
        "4. MIMD stands for Multiple Instruction, Multiple Data. It's a computer architecture where multiple processors can execute different instructions on different data simultaneously.\n",
        "\n",
        "\n",
        "CUDA follows different architecure known as SIMD.\n",
        "SIMD stands for single instruction multiple threads.\n",
        "\n",
        "SIMD stands for Single Instruction, Multiple Data. It's a parallel processing technique where a single instruction is applied to multiple data elements simultaneously, typically using special wide registers.\n",
        "\n",
        "In CUDA\n",
        "- thread blocks is going to execute in single SM. Multiple thread block can be execute simultaneously on same SM depending on resoucre limitation on SM\n",
        "- But one thread block cannot be executing in multiple SM. If device cannot run single block in one SM, then error will return for that kernel launch.\n",
        "\n",
        "SM : In CUDA, SM stands for Streaming Multiprocessor, a fundamental processing unit on NVIDIA GPUs. It's essentially a general-purpose processor that executes CUDA kernels in parallel, responsible for processing data through the GPU.\n",
        "\n",
        "\n",
        "The equivalent of software to hardware is\n",
        "\n",
        "|| Thread --- CUDA Core ||\n",
        "\n",
        "|| Thread block --- SM ||\n",
        "\n",
        "|| Grid --- Device ||"
      ],
      "metadata": {
        "id": "tAw1t5USAQHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Warps\n",
        "\n",
        "From hardware point of view all threads cannot run in parallel. In software we assume that, but hardware has limitations.\n",
        "\n",
        "One block runs in one streamming Multiprocessor (SM).\n",
        "\n",
        "- thread blocks are divide in to smaller units called warps each having 32 consecutive threads.\n",
        "\n",
        "No of warps per block = Block size / Warp size\n",
        "\n",
        "Warps are basic unit of execution in a SM.\n",
        "\n",
        "Once a thread block is scheduled to an SM, threads in the thread block are further partitioned into warps.\n",
        "\n",
        "And all threads in a warp are executed in Single Instruction Multiple thread fashion.\n",
        "\n",
        "Each thread has a unique ID.\n",
        "\n",
        "Warp size is 32 and even to run thread block with single thread, still CUDA runtime will assign single warp which means 32 threads. In this case only 1 thread will be active and other 31 threads will be in inactive state.\n",
        "\n",
        "resoucre allocations like shared memory for block will be done considering number of warps.\n",
        "\n",
        "having inactive threads in warp will be a greate waste of resoucre in SM.\n",
        "\n",
        "There are no any build varaible to indiicate the warp index. But we can get it bby divind threadId.x value by warp size, 32.\n"
      ],
      "metadata": {
        "id": "0xPXbB5QAP9t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpyM7ziwABOM",
        "outputId": "a58403af-4f25-4621-e7d1-66ceb232b0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile hello.cu\n",
        "#include <cstdio>\n",
        "\n",
        "__global__ void print_details_of_warps() {\n",
        "\n",
        "  int gid = blockIdx.y * gridDim.x * blockDim.x + blockDim.x + threadIdx.x ;\n",
        "  int warp_id = threadIdx.x / 32;\n",
        "  int gbid = blockIdx.y * gridDim.x + blockIdx.x ;\n",
        "\n",
        "  printf(\"tid : %d, bid.x : %d , bid.y : %d, gid: %d, warp_id: %d, gbid: %d \\n\",\n",
        "        threadIdx.x, blockIdx.x, blockIdx.y, gid, warp_id, gbid);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    dim3 block_size(42);\n",
        "    dim3 grid_size(2, 2);\n",
        "\n",
        "    print_details_of_warps <<<grid_size, block_size>>> ();\n",
        "    // Synchronize the device to make sure the output is flushed\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hello.cu -o hello -gencode arch=compute_75,code=sm_75\n",
        "!./hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zNd6af1AOlc",
        "outputId": "a317b488-ac50-4a7a-fd1e-bea0e0fac6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tid : 32, bid.x : 0 , bid.y : 1, gid: 158, warp_id: 1, gbid: 2 \n",
            "tid : 33, bid.x : 0 , bid.y : 1, gid: 159, warp_id: 1, gbid: 2 \n",
            "tid : 34, bid.x : 0 , bid.y : 1, gid: 160, warp_id: 1, gbid: 2 \n",
            "tid : 35, bid.x : 0 , bid.y : 1, gid: 161, warp_id: 1, gbid: 2 \n",
            "tid : 36, bid.x : 0 , bid.y : 1, gid: 162, warp_id: 1, gbid: 2 \n",
            "tid : 37, bid.x : 0 , bid.y : 1, gid: 163, warp_id: 1, gbid: 2 \n",
            "tid : 38, bid.x : 0 , bid.y : 1, gid: 164, warp_id: 1, gbid: 2 \n",
            "tid : 39, bid.x : 0 , bid.y : 1, gid: 165, warp_id: 1, gbid: 2 \n",
            "tid : 40, bid.x : 0 , bid.y : 1, gid: 166, warp_id: 1, gbid: 2 \n",
            "tid : 41, bid.x : 0 , bid.y : 1, gid: 167, warp_id: 1, gbid: 2 \n",
            "tid : 32, bid.x : 0 , bid.y : 0, gid: 74, warp_id: 1, gbid: 0 \n",
            "tid : 33, bid.x : 0 , bid.y : 0, gid: 75, warp_id: 1, gbid: 0 \n",
            "tid : 34, bid.x : 0 , bid.y : 0, gid: 76, warp_id: 1, gbid: 0 \n",
            "tid : 35, bid.x : 0 , bid.y : 0, gid: 77, warp_id: 1, gbid: 0 \n",
            "tid : 36, bid.x : 0 , bid.y : 0, gid: 78, warp_id: 1, gbid: 0 \n",
            "tid : 37, bid.x : 0 , bid.y : 0, gid: 79, warp_id: 1, gbid: 0 \n",
            "tid : 38, bid.x : 0 , bid.y : 0, gid: 80, warp_id: 1, gbid: 0 \n",
            "tid : 39, bid.x : 0 , bid.y : 0, gid: 81, warp_id: 1, gbid: 0 \n",
            "tid : 40, bid.x : 0 , bid.y : 0, gid: 82, warp_id: 1, gbid: 0 \n",
            "tid : 41, bid.x : 0 , bid.y : 0, gid: 83, warp_id: 1, gbid: 0 \n",
            "tid : 32, bid.x : 1 , bid.y : 1, gid: 158, warp_id: 1, gbid: 3 \n",
            "tid : 33, bid.x : 1 , bid.y : 1, gid: 159, warp_id: 1, gbid: 3 \n",
            "tid : 34, bid.x : 1 , bid.y : 1, gid: 160, warp_id: 1, gbid: 3 \n",
            "tid : 35, bid.x : 1 , bid.y : 1, gid: 161, warp_id: 1, gbid: 3 \n",
            "tid : 36, bid.x : 1 , bid.y : 1, gid: 162, warp_id: 1, gbid: 3 \n",
            "tid : 37, bid.x : 1 , bid.y : 1, gid: 163, warp_id: 1, gbid: 3 \n",
            "tid : 38, bid.x : 1 , bid.y : 1, gid: 164, warp_id: 1, gbid: 3 \n",
            "tid : 39, bid.x : 1 , bid.y : 1, gid: 165, warp_id: 1, gbid: 3 \n",
            "tid : 40, bid.x : 1 , bid.y : 1, gid: 166, warp_id: 1, gbid: 3 \n",
            "tid : 41, bid.x : 1 , bid.y : 1, gid: 167, warp_id: 1, gbid: 3 \n",
            "tid : 32, bid.x : 1 , bid.y : 0, gid: 74, warp_id: 1, gbid: 1 \n",
            "tid : 33, bid.x : 1 , bid.y : 0, gid: 75, warp_id: 1, gbid: 1 \n",
            "tid : 34, bid.x : 1 , bid.y : 0, gid: 76, warp_id: 1, gbid: 1 \n",
            "tid : 35, bid.x : 1 , bid.y : 0, gid: 77, warp_id: 1, gbid: 1 \n",
            "tid : 36, bid.x : 1 , bid.y : 0, gid: 78, warp_id: 1, gbid: 1 \n",
            "tid : 37, bid.x : 1 , bid.y : 0, gid: 79, warp_id: 1, gbid: 1 \n",
            "tid : 38, bid.x : 1 , bid.y : 0, gid: 80, warp_id: 1, gbid: 1 \n",
            "tid : 39, bid.x : 1 , bid.y : 0, gid: 81, warp_id: 1, gbid: 1 \n",
            "tid : 40, bid.x : 1 , bid.y : 0, gid: 82, warp_id: 1, gbid: 1 \n",
            "tid : 41, bid.x : 1 , bid.y : 0, gid: 83, warp_id: 1, gbid: 1 \n",
            "tid : 0, bid.x : 0 , bid.y : 1, gid: 126, warp_id: 0, gbid: 2 \n",
            "tid : 1, bid.x : 0 , bid.y : 1, gid: 127, warp_id: 0, gbid: 2 \n",
            "tid : 2, bid.x : 0 , bid.y : 1, gid: 128, warp_id: 0, gbid: 2 \n",
            "tid : 3, bid.x : 0 , bid.y : 1, gid: 129, warp_id: 0, gbid: 2 \n",
            "tid : 4, bid.x : 0 , bid.y : 1, gid: 130, warp_id: 0, gbid: 2 \n",
            "tid : 5, bid.x : 0 , bid.y : 1, gid: 131, warp_id: 0, gbid: 2 \n",
            "tid : 6, bid.x : 0 , bid.y : 1, gid: 132, warp_id: 0, gbid: 2 \n",
            "tid : 7, bid.x : 0 , bid.y : 1, gid: 133, warp_id: 0, gbid: 2 \n",
            "tid : 8, bid.x : 0 , bid.y : 1, gid: 134, warp_id: 0, gbid: 2 \n",
            "tid : 9, bid.x : 0 , bid.y : 1, gid: 135, warp_id: 0, gbid: 2 \n",
            "tid : 10, bid.x : 0 , bid.y : 1, gid: 136, warp_id: 0, gbid: 2 \n",
            "tid : 11, bid.x : 0 , bid.y : 1, gid: 137, warp_id: 0, gbid: 2 \n",
            "tid : 12, bid.x : 0 , bid.y : 1, gid: 138, warp_id: 0, gbid: 2 \n",
            "tid : 13, bid.x : 0 , bid.y : 1, gid: 139, warp_id: 0, gbid: 2 \n",
            "tid : 14, bid.x : 0 , bid.y : 1, gid: 140, warp_id: 0, gbid: 2 \n",
            "tid : 15, bid.x : 0 , bid.y : 1, gid: 141, warp_id: 0, gbid: 2 \n",
            "tid : 16, bid.x : 0 , bid.y : 1, gid: 142, warp_id: 0, gbid: 2 \n",
            "tid : 17, bid.x : 0 , bid.y : 1, gid: 143, warp_id: 0, gbid: 2 \n",
            "tid : 18, bid.x : 0 , bid.y : 1, gid: 144, warp_id: 0, gbid: 2 \n",
            "tid : 19, bid.x : 0 , bid.y : 1, gid: 145, warp_id: 0, gbid: 2 \n",
            "tid : 20, bid.x : 0 , bid.y : 1, gid: 146, warp_id: 0, gbid: 2 \n",
            "tid : 21, bid.x : 0 , bid.y : 1, gid: 147, warp_id: 0, gbid: 2 \n",
            "tid : 22, bid.x : 0 , bid.y : 1, gid: 148, warp_id: 0, gbid: 2 \n",
            "tid : 23, bid.x : 0 , bid.y : 1, gid: 149, warp_id: 0, gbid: 2 \n",
            "tid : 24, bid.x : 0 , bid.y : 1, gid: 150, warp_id: 0, gbid: 2 \n",
            "tid : 25, bid.x : 0 , bid.y : 1, gid: 151, warp_id: 0, gbid: 2 \n",
            "tid : 26, bid.x : 0 , bid.y : 1, gid: 152, warp_id: 0, gbid: 2 \n",
            "tid : 27, bid.x : 0 , bid.y : 1, gid: 153, warp_id: 0, gbid: 2 \n",
            "tid : 28, bid.x : 0 , bid.y : 1, gid: 154, warp_id: 0, gbid: 2 \n",
            "tid : 29, bid.x : 0 , bid.y : 1, gid: 155, warp_id: 0, gbid: 2 \n",
            "tid : 30, bid.x : 0 , bid.y : 1, gid: 156, warp_id: 0, gbid: 2 \n",
            "tid : 31, bid.x : 0 , bid.y : 1, gid: 157, warp_id: 0, gbid: 2 \n",
            "tid : 0, bid.x : 0 , bid.y : 0, gid: 42, warp_id: 0, gbid: 0 \n",
            "tid : 1, bid.x : 0 , bid.y : 0, gid: 43, warp_id: 0, gbid: 0 \n",
            "tid : 2, bid.x : 0 , bid.y : 0, gid: 44, warp_id: 0, gbid: 0 \n",
            "tid : 3, bid.x : 0 , bid.y : 0, gid: 45, warp_id: 0, gbid: 0 \n",
            "tid : 4, bid.x : 0 , bid.y : 0, gid: 46, warp_id: 0, gbid: 0 \n",
            "tid : 5, bid.x : 0 , bid.y : 0, gid: 47, warp_id: 0, gbid: 0 \n",
            "tid : 6, bid.x : 0 , bid.y : 0, gid: 48, warp_id: 0, gbid: 0 \n",
            "tid : 7, bid.x : 0 , bid.y : 0, gid: 49, warp_id: 0, gbid: 0 \n",
            "tid : 8, bid.x : 0 , bid.y : 0, gid: 50, warp_id: 0, gbid: 0 \n",
            "tid : 9, bid.x : 0 , bid.y : 0, gid: 51, warp_id: 0, gbid: 0 \n",
            "tid : 10, bid.x : 0 , bid.y : 0, gid: 52, warp_id: 0, gbid: 0 \n",
            "tid : 11, bid.x : 0 , bid.y : 0, gid: 53, warp_id: 0, gbid: 0 \n",
            "tid : 12, bid.x : 0 , bid.y : 0, gid: 54, warp_id: 0, gbid: 0 \n",
            "tid : 13, bid.x : 0 , bid.y : 0, gid: 55, warp_id: 0, gbid: 0 \n",
            "tid : 14, bid.x : 0 , bid.y : 0, gid: 56, warp_id: 0, gbid: 0 \n",
            "tid : 15, bid.x : 0 , bid.y : 0, gid: 57, warp_id: 0, gbid: 0 \n",
            "tid : 16, bid.x : 0 , bid.y : 0, gid: 58, warp_id: 0, gbid: 0 \n",
            "tid : 17, bid.x : 0 , bid.y : 0, gid: 59, warp_id: 0, gbid: 0 \n",
            "tid : 18, bid.x : 0 , bid.y : 0, gid: 60, warp_id: 0, gbid: 0 \n",
            "tid : 19, bid.x : 0 , bid.y : 0, gid: 61, warp_id: 0, gbid: 0 \n",
            "tid : 20, bid.x : 0 , bid.y : 0, gid: 62, warp_id: 0, gbid: 0 \n",
            "tid : 21, bid.x : 0 , bid.y : 0, gid: 63, warp_id: 0, gbid: 0 \n",
            "tid : 22, bid.x : 0 , bid.y : 0, gid: 64, warp_id: 0, gbid: 0 \n",
            "tid : 23, bid.x : 0 , bid.y : 0, gid: 65, warp_id: 0, gbid: 0 \n",
            "tid : 24, bid.x : 0 , bid.y : 0, gid: 66, warp_id: 0, gbid: 0 \n",
            "tid : 25, bid.x : 0 , bid.y : 0, gid: 67, warp_id: 0, gbid: 0 \n",
            "tid : 26, bid.x : 0 , bid.y : 0, gid: 68, warp_id: 0, gbid: 0 \n",
            "tid : 27, bid.x : 0 , bid.y : 0, gid: 69, warp_id: 0, gbid: 0 \n",
            "tid : 28, bid.x : 0 , bid.y : 0, gid: 70, warp_id: 0, gbid: 0 \n",
            "tid : 29, bid.x : 0 , bid.y : 0, gid: 71, warp_id: 0, gbid: 0 \n",
            "tid : 30, bid.x : 0 , bid.y : 0, gid: 72, warp_id: 0, gbid: 0 \n",
            "tid : 31, bid.x : 0 , bid.y : 0, gid: 73, warp_id: 0, gbid: 0 \n",
            "tid : 0, bid.x : 1 , bid.y : 1, gid: 126, warp_id: 0, gbid: 3 \n",
            "tid : 1, bid.x : 1 , bid.y : 1, gid: 127, warp_id: 0, gbid: 3 \n",
            "tid : 2, bid.x : 1 , bid.y : 1, gid: 128, warp_id: 0, gbid: 3 \n",
            "tid : 3, bid.x : 1 , bid.y : 1, gid: 129, warp_id: 0, gbid: 3 \n",
            "tid : 4, bid.x : 1 , bid.y : 1, gid: 130, warp_id: 0, gbid: 3 \n",
            "tid : 5, bid.x : 1 , bid.y : 1, gid: 131, warp_id: 0, gbid: 3 \n",
            "tid : 6, bid.x : 1 , bid.y : 1, gid: 132, warp_id: 0, gbid: 3 \n",
            "tid : 7, bid.x : 1 , bid.y : 1, gid: 133, warp_id: 0, gbid: 3 \n",
            "tid : 8, bid.x : 1 , bid.y : 1, gid: 134, warp_id: 0, gbid: 3 \n",
            "tid : 9, bid.x : 1 , bid.y : 1, gid: 135, warp_id: 0, gbid: 3 \n",
            "tid : 10, bid.x : 1 , bid.y : 1, gid: 136, warp_id: 0, gbid: 3 \n",
            "tid : 11, bid.x : 1 , bid.y : 1, gid: 137, warp_id: 0, gbid: 3 \n",
            "tid : 12, bid.x : 1 , bid.y : 1, gid: 138, warp_id: 0, gbid: 3 \n",
            "tid : 13, bid.x : 1 , bid.y : 1, gid: 139, warp_id: 0, gbid: 3 \n",
            "tid : 14, bid.x : 1 , bid.y : 1, gid: 140, warp_id: 0, gbid: 3 \n",
            "tid : 15, bid.x : 1 , bid.y : 1, gid: 141, warp_id: 0, gbid: 3 \n",
            "tid : 16, bid.x : 1 , bid.y : 1, gid: 142, warp_id: 0, gbid: 3 \n",
            "tid : 17, bid.x : 1 , bid.y : 1, gid: 143, warp_id: 0, gbid: 3 \n",
            "tid : 18, bid.x : 1 , bid.y : 1, gid: 144, warp_id: 0, gbid: 3 \n",
            "tid : 19, bid.x : 1 , bid.y : 1, gid: 145, warp_id: 0, gbid: 3 \n",
            "tid : 20, bid.x : 1 , bid.y : 1, gid: 146, warp_id: 0, gbid: 3 \n",
            "tid : 21, bid.x : 1 , bid.y : 1, gid: 147, warp_id: 0, gbid: 3 \n",
            "tid : 22, bid.x : 1 , bid.y : 1, gid: 148, warp_id: 0, gbid: 3 \n",
            "tid : 23, bid.x : 1 , bid.y : 1, gid: 149, warp_id: 0, gbid: 3 \n",
            "tid : 24, bid.x : 1 , bid.y : 1, gid: 150, warp_id: 0, gbid: 3 \n",
            "tid : 25, bid.x : 1 , bid.y : 1, gid: 151, warp_id: 0, gbid: 3 \n",
            "tid : 26, bid.x : 1 , bid.y : 1, gid: 152, warp_id: 0, gbid: 3 \n",
            "tid : 27, bid.x : 1 , bid.y : 1, gid: 153, warp_id: 0, gbid: 3 \n",
            "tid : 28, bid.x : 1 , bid.y : 1, gid: 154, warp_id: 0, gbid: 3 \n",
            "tid : 29, bid.x : 1 , bid.y : 1, gid: 155, warp_id: 0, gbid: 3 \n",
            "tid : 30, bid.x : 1 , bid.y : 1, gid: 156, warp_id: 0, gbid: 3 \n",
            "tid : 31, bid.x : 1 , bid.y : 1, gid: 157, warp_id: 0, gbid: 3 \n",
            "tid : 0, bid.x : 1 , bid.y : 0, gid: 42, warp_id: 0, gbid: 1 \n",
            "tid : 1, bid.x : 1 , bid.y : 0, gid: 43, warp_id: 0, gbid: 1 \n",
            "tid : 2, bid.x : 1 , bid.y : 0, gid: 44, warp_id: 0, gbid: 1 \n",
            "tid : 3, bid.x : 1 , bid.y : 0, gid: 45, warp_id: 0, gbid: 1 \n",
            "tid : 4, bid.x : 1 , bid.y : 0, gid: 46, warp_id: 0, gbid: 1 \n",
            "tid : 5, bid.x : 1 , bid.y : 0, gid: 47, warp_id: 0, gbid: 1 \n",
            "tid : 6, bid.x : 1 , bid.y : 0, gid: 48, warp_id: 0, gbid: 1 \n",
            "tid : 7, bid.x : 1 , bid.y : 0, gid: 49, warp_id: 0, gbid: 1 \n",
            "tid : 8, bid.x : 1 , bid.y : 0, gid: 50, warp_id: 0, gbid: 1 \n",
            "tid : 9, bid.x : 1 , bid.y : 0, gid: 51, warp_id: 0, gbid: 1 \n",
            "tid : 10, bid.x : 1 , bid.y : 0, gid: 52, warp_id: 0, gbid: 1 \n",
            "tid : 11, bid.x : 1 , bid.y : 0, gid: 53, warp_id: 0, gbid: 1 \n",
            "tid : 12, bid.x : 1 , bid.y : 0, gid: 54, warp_id: 0, gbid: 1 \n",
            "tid : 13, bid.x : 1 , bid.y : 0, gid: 55, warp_id: 0, gbid: 1 \n",
            "tid : 14, bid.x : 1 , bid.y : 0, gid: 56, warp_id: 0, gbid: 1 \n",
            "tid : 15, bid.x : 1 , bid.y : 0, gid: 57, warp_id: 0, gbid: 1 \n",
            "tid : 16, bid.x : 1 , bid.y : 0, gid: 58, warp_id: 0, gbid: 1 \n",
            "tid : 17, bid.x : 1 , bid.y : 0, gid: 59, warp_id: 0, gbid: 1 \n",
            "tid : 18, bid.x : 1 , bid.y : 0, gid: 60, warp_id: 0, gbid: 1 \n",
            "tid : 19, bid.x : 1 , bid.y : 0, gid: 61, warp_id: 0, gbid: 1 \n",
            "tid : 20, bid.x : 1 , bid.y : 0, gid: 62, warp_id: 0, gbid: 1 \n",
            "tid : 21, bid.x : 1 , bid.y : 0, gid: 63, warp_id: 0, gbid: 1 \n",
            "tid : 22, bid.x : 1 , bid.y : 0, gid: 64, warp_id: 0, gbid: 1 \n",
            "tid : 23, bid.x : 1 , bid.y : 0, gid: 65, warp_id: 0, gbid: 1 \n",
            "tid : 24, bid.x : 1 , bid.y : 0, gid: 66, warp_id: 0, gbid: 1 \n",
            "tid : 25, bid.x : 1 , bid.y : 0, gid: 67, warp_id: 0, gbid: 1 \n",
            "tid : 26, bid.x : 1 , bid.y : 0, gid: 68, warp_id: 0, gbid: 1 \n",
            "tid : 27, bid.x : 1 , bid.y : 0, gid: 69, warp_id: 0, gbid: 1 \n",
            "tid : 28, bid.x : 1 , bid.y : 0, gid: 70, warp_id: 0, gbid: 1 \n",
            "tid : 29, bid.x : 1 , bid.y : 0, gid: 71, warp_id: 0, gbid: 1 \n",
            "tid : 30, bid.x : 1 , bid.y : 0, gid: 72, warp_id: 0, gbid: 1 \n",
            "tid : 31, bid.x : 1 , bid.y : 0, gid: 73, warp_id: 0, gbid: 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Warp Divergence\n",
        "\n",
        "Warp divergence occurs when threads within the same warp follow different execution paths through a conditional branch. Since all threads in a warp must execute the same instruction at any given cycle, divergent paths are executed serially—one path at a time—masking out threads not taking that path.\n",
        "\n",
        "- Warp is diverge when there is multiple path of execution with in same warp. So condition checks, which result in all thread executing same path, will not induce any wrap divergence."
      ],
      "metadata": {
        "id": "S2Xl4XoORFKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if (tid % 2 != 0)\n",
        "{\n",
        "  //do something\n",
        "}\n",
        "else\n",
        "{\n",
        "  // do something else\n",
        "}\n",
        "\n",
        "\n",
        "In the above statment there are 2 branches.\n",
        "\n",
        "$Branch Efficiency = 100 * ( #Branches - #Divergent_Branches  / #Branches)$\n",
        "\n",
        "$= 100 * (2-1) / 2 = 50%$"
      ],
      "metadata": {
        "id": "nDKmm327wWPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello.cu\n",
        "#include <cstdio>\n",
        "\n",
        "__global__ void code_without_divergence() {\n",
        "\n",
        "  int gid = blockIdx.x * blockDim.x + threadIdx.x ;\n",
        "  float a, b;\n",
        "  a=b=0;\n",
        "  int warp_id = gid / 32;\n",
        "  if (warp_id%2 ==0){\n",
        "    a = 100.0;\n",
        "    b= 50.0;\n",
        "  }\n",
        "  else{\n",
        "    a = 200;\n",
        "    b = 75;\n",
        "  }\n",
        "  // Add dummy computation\n",
        "  for (int i = 0; i < 1000; ++i) {\n",
        "        a = a * 1.00001f + b * 0.99999f;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void divergence_code() { // consecutive threads will take difference part of executions\n",
        "\n",
        "  int gid = blockIdx.x * blockDim.x + threadIdx.x ;\n",
        "  float a, b;\n",
        "  a=b=0;\n",
        "  if (gid%2 ==0){\n",
        "    a = 100.0;\n",
        "    b= 50.0;\n",
        "  }\n",
        "  else{\n",
        "    a = 200;\n",
        "    b = 75;\n",
        "  }\n",
        "  // Add dummy computation\n",
        "  for (int i = 0; i < 1000; ++i) {\n",
        "        a = a * 1.00001f + b * 0.99999f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  printf(\"\\n-----------------------WARP DIVERGENCE EXAMPLE-------------------------------\\n\\n\");\n",
        "  int size = 1<<22;\n",
        "  dim3 block_size(128);\n",
        "  dim3 grid_size((size + block_size.x - 1) / block_size.x);\n",
        "\n",
        "  code_without_divergence<<<grid_size,block_size>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "  divergence_code<<<grid_size,block_size>>>();\n",
        "  // Synchronize the device to make sure the output is flushed\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "3aiXRHAPJRqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2289de8c-2990-4985-8dbf-fbd9d8602693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hello.cu -o hello.out -gencode arch=compute_75,code=sm_75\n",
        "!./hello.out   # To just run the program\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NogOm3tqvS88",
        "outputId": "63799a12-8f50-4755-d025-b48e4a548711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------WARP DIVERGENCE EXAMPLE-------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hello.cu -o hello.out -gencode arch=compute_75,code=sm_75\n",
        "!ncu --metrics sm__branch_efficiency.avg.pct ./hello.out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn_XnyPZx924",
        "outputId": "40390ba0-41fa-4bdd-e95a-104a2a0623ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------WARP DIVERGENCE EXAMPLE-------------------------------\n",
            "\n",
            "==PROF== Connected to process 473 (/content/hello.out)\n",
            "==PROF== Profiling \"code_without_divergence()\" - 0: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"divergence_code()\" - 1: 0%....50%....100% - 1 pass\n",
            "==PROF== Disconnected from process 473\n",
            "[473] hello.out@127.0.0.1\n",
            "  code_without_divergence() (32768, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ----------------------------- ----------- ------------\n",
            "    Metric Name                   Metric Unit Metric Value\n",
            "    ----------------------------- ----------- ------------\n",
            "    sm__branch_efficiency.avg.pct                  (!) n/a\n",
            "    ----------------------------- ----------- ------------\n",
            "\n",
            "  divergence_code() (32768, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ----------------------------- ----------- ------------\n",
            "    Metric Name                   Metric Unit Metric Value\n",
            "    ----------------------------- ----------- ------------\n",
            "    sm__branch_efficiency.avg.pct                  (!) n/a\n",
            "    ----------------------------- ----------- ------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --set full ./hello.out\n",
        "## full wrap divergence details are shown below:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGnuQBR-yfkE",
        "outputId": "550390ac-4e81-4bd0-cf1c-febdad81e1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------WARP DIVERGENCE EXAMPLE-------------------------------\n",
            "\n",
            "==PROF== Connected to process 513 (/content/hello.out)\n",
            "==PROF== Profiling \"code_without_divergence()\" - 0: 0%....50%....100% - 30 passes\n",
            "==PROF== Profiling \"divergence_code()\" - 1: 0%....50%....100% - 30 passes\n",
            "==PROF== Disconnected from process 513\n",
            "[513] hello.out@127.0.0.1\n",
            "  code_without_divergence() (32768, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.96\n",
            "    SM Frequency                    Mhz       584.58\n",
            "    Elapsed Cycles                cycle       38,819\n",
            "    Memory Throughput                 %         0.07\n",
            "    DRAM Throughput                   %         0.01\n",
            "    Duration                         us        66.40\n",
            "    L1/TEX Cache Throughput           %         0.02\n",
            "    L2 Cache Throughput               %         0.07\n",
            "    SM Active Cycles              cycle    27,624.25\n",
            "    Compute (SM) Throughput           %         8.57\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n",
            "          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n",
            "          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \n",
            "\n",
            "    Section: GPU Speed Of Light Roofline Chart\n",
            "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n",
            "          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n",
            "          analysis.                                                                                                     \n",
            "\n",
            "    Section: PM Sampling\n",
            "    ------------------------- ----------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ------------------------- ----------- ------------\n",
            "    Maximum Buffer Size             Kbyte        65.54\n",
            "    Dropped Samples                sample            0\n",
            "    Maximum Sampling Interval       cycle       20,000\n",
            "    # Pass Groups                                    1\n",
            "    ------------------------- ----------- ------------\n",
            "\n",
            "    Section: Compute Workload Analysis\n",
            "    -------------------- ----------- ------------\n",
            "    Metric Name          Metric Unit Metric Value\n",
            "    -------------------- ----------- ------------\n",
            "    Executed Ipc Active   inst/cycle         0.24\n",
            "    Executed Ipc Elapsed  inst/cycle         0.17\n",
            "    Issue Slots Busy               %         5.98\n",
            "    Issued Ipc Active     inst/cycle         0.24\n",
            "    SM Busy                        %        11.86\n",
            "    -------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 94.07%                                                                                    \n",
            "          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n",
            "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
            "\n",
            "    Section: Memory Workload Analysis\n",
            "    ----------------- ----------- ------------\n",
            "    Metric Name       Metric Unit Metric Value\n",
            "    ----------------- ----------- ------------\n",
            "    Memory Throughput     Mbyte/s        41.45\n",
            "    Mem Busy                    %         0.07\n",
            "    Max Bandwidth               %         0.03\n",
            "    L1/TEX Hit Rate             %            0\n",
            "    L2 Hit Rate                 %        71.45\n",
            "    Mem Pipes Busy              %            0\n",
            "    ----------------- ----------- ------------\n",
            "\n",
            "    Section: Memory Workload Analysis Chart\n",
            "    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n",
            "          additional metric could enable the rule to provide more guidance.                                             \n",
            "\n",
            "    Section: Scheduler Statistics\n",
            "    ---------------------------- ----------- ------------\n",
            "    Metric Name                  Metric Unit Metric Value\n",
            "    ---------------------------- ----------- ------------\n",
            "    One or More Eligible                   %         8.87\n",
            "    Issued Warp Per Scheduler                        0.09\n",
            "    No Eligible                            %        91.13\n",
            "    Active Warps Per Scheduler          warp         1.12\n",
            "    Eligible Warps Per Scheduler        warp         0.09\n",
            "    ---------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 91.13%                                                                                    \n",
            "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
            "          issues an instruction every 11.3 cycles. This might leave hardware resources underutilized and may lead to    \n",
            "          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n",
            "          1.12 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps    \n",
            "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
            "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
            "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
            "          State Statistics and Source Counters sections.                                                                \n",
            "\n",
            "    Section: Warp State Statistics\n",
            "    ---------------------------------------- ----------- ------------\n",
            "    Metric Name                              Metric Unit Metric Value\n",
            "    ---------------------------------------- ----------- ------------\n",
            "    Warp Cycles Per Issued Instruction             cycle        12.59\n",
            "    Warp Cycles Per Executed Instruction           cycle        12.68\n",
            "    Avg. Active Threads Per Warp                                   32\n",
            "    Avg. Not Predicated Off Threads Per Warp                       32\n",
            "    ---------------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Speedup: 32.49%                                                                                          \n",
            "          On average, each warp of this kernel spends 4.1 cycles being stalled waiting for a branch target to be        \n",
            "          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  \n",
            "          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  \n",
            "          in your code. See also the related No Instructions state. This stall type represents about 32.5% of the       \n",
            "          total average of 12.6 cycles between issuing two instructions.                                                \n",
            "    ----- --------------------------------------------------------------------------------------------------------------\n",
            "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
            "          sampling data. The Kernel Profiling Guide                                                                     \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
            "          on each stall reason.                                                                                         \n",
            "\n",
            "    Section: Instruction Statistics\n",
            "    ---------------------------------------- ----------- ------------\n",
            "    Metric Name                              Metric Unit Metric Value\n",
            "    ---------------------------------------- ----------- ------------\n",
            "    Avg. Executed Instructions Per Scheduler        inst     1,638.40\n",
            "    Executed Instructions                           inst      262,144\n",
            "    Avg. Issued Instructions Per Scheduler          inst     1,650.97\n",
            "    Issued Instructions                             inst      264,156\n",
            "    ---------------------------------------- ----------- ------------\n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   128\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                 32,768\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       4,194,304\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                              102.40\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           32\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            8\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        16.33\n",
            "    Achieved Active Warps Per SM           warp         5.22\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Speedup: 83.67%                                                                                          \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.3%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle           43\n",
            "    Total DRAM Elapsed Cycles        cycle    2,635,776\n",
            "    Average L1 Active Cycles         cycle    27,624.25\n",
            "    Total L1 Elapsed Cycles          cycle    1,529,712\n",
            "    Average L2 Active Cycles         cycle       214.25\n",
            "    Total L2 Elapsed Cycles          cycle    1,815,520\n",
            "    Average SM Active Cycles         cycle    27,624.25\n",
            "    Total SM Elapsed Cycles          cycle    1,529,712\n",
            "    Average SMSP Active Cycles       cycle    18,623.31\n",
            "    Total SMSP Elapsed Cycles        cycle    6,118,848\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "    Section: Source Counters\n",
            "    ------------------------- ----------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ------------------------- ----------- ------------\n",
            "    Branch Instructions Ratio           %         0.50\n",
            "    Branch Instructions              inst      131,072\n",
            "    Branch Efficiency                   %            0\n",
            "    Avg. Divergent Branches                          0\n",
            "    ------------------------- ----------- ------------\n",
            "\n",
            "  divergence_code() (32768, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz            5\n",
            "    SM Frequency                    Mhz       584.20\n",
            "    Elapsed Cycles                cycle       38,736\n",
            "    Memory Throughput                 %         0.05\n",
            "    DRAM Throughput                   %         0.01\n",
            "    Duration                         us        66.30\n",
            "    L1/TEX Cache Throughput           %         0.02\n",
            "    L2 Cache Throughput               %         0.05\n",
            "    SM Active Cycles              cycle    27,469.60\n",
            "    Compute (SM) Throughput           %         8.56\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n",
            "          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n",
            "          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \n",
            "\n",
            "    Section: GPU Speed Of Light Roofline Chart\n",
            "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n",
            "          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n",
            "          analysis.                                                                                                     \n",
            "\n",
            "    Section: PM Sampling\n",
            "    ------------------------- ----------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ------------------------- ----------- ------------\n",
            "    Maximum Buffer Size             Kbyte        65.54\n",
            "    Dropped Samples                sample            0\n",
            "    Maximum Sampling Interval       cycle       20,000\n",
            "    # Pass Groups                                    1\n",
            "    ------------------------- ----------- ------------\n",
            "\n",
            "    Section: Compute Workload Analysis\n",
            "    -------------------- ----------- ------------\n",
            "    Metric Name          Metric Unit Metric Value\n",
            "    -------------------- ----------- ------------\n",
            "    Executed Ipc Active   inst/cycle         0.24\n",
            "    Executed Ipc Elapsed  inst/cycle         0.17\n",
            "    Issue Slots Busy               %         6.01\n",
            "    Issued Ipc Active     inst/cycle         0.24\n",
            "    SM Busy                        %        11.93\n",
            "    -------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 94.04%                                                                                    \n",
            "          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n",
            "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
            "\n",
            "    Section: Memory Workload Analysis\n",
            "    ----------------- ----------- ------------\n",
            "    Metric Name       Metric Unit Metric Value\n",
            "    ----------------- ----------- ------------\n",
            "    Memory Throughput     Mbyte/s        41.51\n",
            "    Mem Busy                    %         0.05\n",
            "    Max Bandwidth               %         0.03\n",
            "    L1/TEX Hit Rate             %            0\n",
            "    L2 Hit Rate                 %        92.86\n",
            "    Mem Pipes Busy              %            0\n",
            "    ----------------- ----------- ------------\n",
            "\n",
            "    Section: Memory Workload Analysis Chart\n",
            "    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n",
            "          additional metric could enable the rule to provide more guidance.                                             \n",
            "\n",
            "    Section: Scheduler Statistics\n",
            "    ---------------------------- ----------- ------------\n",
            "    Metric Name                  Metric Unit Metric Value\n",
            "    ---------------------------- ----------- ------------\n",
            "    One or More Eligible                   %         8.85\n",
            "    Issued Warp Per Scheduler                        0.09\n",
            "    No Eligible                            %        91.15\n",
            "    Active Warps Per Scheduler          warp         1.11\n",
            "    Eligible Warps Per Scheduler        warp         0.09\n",
            "    ---------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 91.15%                                                                                    \n",
            "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
            "          issues an instruction every 11.3 cycles. This might leave hardware resources underutilized and may lead to    \n",
            "          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n",
            "          1.11 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps    \n",
            "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
            "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
            "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
            "          State Statistics and Source Counters sections.                                                                \n",
            "\n",
            "    Section: Warp State Statistics\n",
            "    ---------------------------------------- ----------- ------------\n",
            "    Metric Name                              Metric Unit Metric Value\n",
            "    ---------------------------------------- ----------- ------------\n",
            "    Warp Cycles Per Issued Instruction             cycle        12.56\n",
            "    Warp Cycles Per Executed Instruction           cycle        12.66\n",
            "    Avg. Active Threads Per Warp                                   32\n",
            "    Avg. Not Predicated Off Threads Per Warp                       32\n",
            "    ---------------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Speedup: 32.47%                                                                                          \n",
            "          On average, each warp of this kernel spends 4.1 cycles being stalled waiting for a branch target to be        \n",
            "          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  \n",
            "          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  \n",
            "          in your code. See also the related No Instructions state. This stall type represents about 32.5% of the       \n",
            "          total average of 12.6 cycles between issuing two instructions.                                                \n",
            "    ----- --------------------------------------------------------------------------------------------------------------\n",
            "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
            "          sampling data. The Kernel Profiling Guide                                                                     \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
            "          on each stall reason.                                                                                         \n",
            "\n",
            "    Section: Instruction Statistics\n",
            "    ---------------------------------------- ----------- ------------\n",
            "    Metric Name                              Metric Unit Metric Value\n",
            "    ---------------------------------------- ----------- ------------\n",
            "    Avg. Executed Instructions Per Scheduler        inst     1,638.40\n",
            "    Executed Instructions                           inst      262,144\n",
            "    Avg. Issued Instructions Per Scheduler          inst     1,651.09\n",
            "    Issued Instructions                             inst      264,174\n",
            "    ---------------------------------------- ----------- ------------\n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   128\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                 32,768\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       4,194,304\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                              102.40\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           32\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            8\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        16.42\n",
            "    Achieved Active Warps Per SM           warp         5.25\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Speedup: 83.58%                                                                                          \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.4%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle           43\n",
            "    Total DRAM Elapsed Cycles        cycle    2,652,160\n",
            "    Average L1 Active Cycles         cycle    27,469.60\n",
            "    Total L1 Elapsed Cycles          cycle    1,531,216\n",
            "    Average L2 Active Cycles         cycle       147.22\n",
            "    Total L2 Elapsed Cycles          cycle    1,811,584\n",
            "    Average SM Active Cycles         cycle    27,469.60\n",
            "    Total SM Elapsed Cycles          cycle    1,531,216\n",
            "    Average SMSP Active Cycles       cycle    18,660.88\n",
            "    Total SMSP Elapsed Cycles        cycle    6,124,864\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Speedup: 5.958%                                                                                          \n",
            "          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n",
            "          instance value is 8.30% above the average, while the minimum instance value is 14.48% below the average.      \n",
            "    ----- --------------------------------------------------------------------------------------------------------------\n",
            "    OPT   Est. Speedup: 5.958%                                                                                          \n",
            "          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n",
            "          Maximum instance value is 8.30% above the average, while the minimum instance value is 14.48% below the       \n",
            "          average.                                                                                                      \n",
            "\n",
            "    Section: Source Counters\n",
            "    ------------------------- ----------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ------------------------- ----------- ------------\n",
            "    Branch Instructions Ratio           %         0.50\n",
            "    Branch Instructions              inst      131,072\n",
            "    Branch Efficiency                   %            0\n",
            "    Avg. Divergent Branches                          0\n",
            "    ------------------------- ----------- ------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resource Partioning and latency hiding\n",
        "\n",
        "The local execution context of a warp mainly consists of the following resources:\n",
        "- Program counters\n",
        "- Registers\n",
        "- Shared memory\n",
        "\n",
        "The execution context of each warp processed by a SM is maintained on-chip during the entire lifteime of the warp. Therefore, swtiching from one execution context to another has no cost.\n",
        "\n",
        "Registers and shared memory can be directly controlled by the programmer.\n",
        "\n",
        "set of 32-bit registers stored in a register file that are partitioned among threads, and a fixed amount of shared memory that is partitioned among thread blocks.\n",
        "\n",
        "Fewer warps with more register per thread -- more warps with fewere register per thread\n",
        "\n",
        "Fewer blocks with more shared memory per block -- more blocks with less shared memory per block\n",
        "\n",
        "Warp categories in SM\n",
        "\n",
        "- Active blocks / warps -- resources have been allocated\n",
        "- Selected warp -- actively executing\n",
        "- Stalled warp -- not ready for execution\n",
        "- Eligible warp -- ready for execution but not currently executing\n",
        "\n",
        "Eligbble for warps\n",
        "\n",
        "- 32 cuda cores should free for execution\n",
        "\n",
        "- all arguments to the current instruction for that warp to be ready\n",
        "\n",
        "What is latency?\n",
        "\n",
        " >number of clock cycles between instruction being issued and being completed\n",
        "\n",
        "- Arithmetic instruction latency\n",
        "- Memory operation latency\n",
        "\n",
        "\n",
        "Latency Hiding:\n",
        "\n",
        "The exeuction context of each warp processed by and SM are maintained on-chip during the entire lifetime of the warp.\n",
        "\n",
        "Therefore switching from one execution context to another has no cost.\n",
        "\n",
        "1 SM -> 128 cores <- can execute 4 warps parallelly in one SM\n",
        "\n",
        "How about memory latency?\n",
        "\n",
        "lets consider DRAM latency of Maxwell architecture as 350 cycles.\n",
        "\n",
        "T4 has 300 GB / s memory bandwidth\n",
        "\n"
      ],
      "metadata": {
        "id": "D7SQJGB5FfuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -a -q -d CLOCK"
      ],
      "metadata": {
        "id": "JtZNNGTyzWfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a0a9b5-f729-4b62-c028-26be9306cc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============NVSMI LOG==============\n",
            "\n",
            "Timestamp                                 : Tue Apr 29 15:18:52 2025\n",
            "Driver Version                            : 550.54.15\n",
            "CUDA Version                              : 12.4\n",
            "\n",
            "Attached GPUs                             : 1\n",
            "GPU 00000000:00:04.0\n",
            "    Clocks\n",
            "        Graphics                          : 300 MHz\n",
            "        SM                                : 300 MHz\n",
            "        Memory                            : 405 MHz\n",
            "        Video                             : 540 MHz\n",
            "    Applications Clocks\n",
            "        Graphics                          : 585 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "    Default Applications Clocks\n",
            "        Graphics                          : 585 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "    Deferred Clocks\n",
            "        Memory                            : N/A\n",
            "    Max Clocks\n",
            "        Graphics                          : 1590 MHz\n",
            "        SM                                : 1590 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "        Video                             : 1470 MHz\n",
            "    Max Customer Boost Clocks\n",
            "        Graphics                          : 1590 MHz\n",
            "    SM Clock Samples\n",
            "        Duration                          : Not Found\n",
            "        Number of Samples                 : Not Found\n",
            "        Max                               : Not Found\n",
            "        Min                               : Not Found\n",
            "        Avg                               : Not Found\n",
            "    Memory Clock Samples\n",
            "        Duration                          : Not Found\n",
            "        Number of Samples                 : Not Found\n",
            "        Max                               : Not Found\n",
            "        Min                               : Not Found\n",
            "        Avg                               : Not Found\n",
            "    Clock Policy\n",
            "        Auto Boost                        : N/A\n",
            "        Auto Boost Default                : N/A\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 GHz memory clock\n",
        "\n",
        "300  / 5 = 60 Bytes / cycle.\n",
        "\n",
        "```\n",
        "<SM1>  <SM2>\n",
        "  |      |\n",
        "  |      |\n",
        "  |------|----> <DRAM>\n",
        "  |      |\n",
        "  |      |\n",
        "<SM3>  <SM4>\n",
        "\n",
        "```\n",
        "\n",
        "60 * 350 = 18400Bytes\n",
        "\n",
        "18400 / 4 = 4600 threads\n",
        "\n",
        "4600 / 32  = 148 warps\n",
        "\n",
        "148 / 13  = 12 warps per SM\n",
        "\n",
        "Categorizing CUDA applications\n",
        "\n",
        "- Bandwidth bound applications -- arithmetic latency\n",
        "- Computation bound applications -- memory latency\n",
        "\n",
        "\n",
        "Occupancy is the ratio of active warps to maximum number of warps, per SM.\n",
        "\n",
        "Occupancy  = $\\frac{Active warps}{maximum warps}$\n",
        "\n",
        "If one warp stalls execution core will be busy\n",
        "\n",
        "maximum warps is found in documentation\n",
        "\n",
        "active warps depend on device usage\n",
        "\n",
        "\n",
        "Our kernel use 48 resiters per thread and 4096 bytes of Smem per block. And block size is 128.\n",
        "\n",
        "Reg per warp = 48 *32 = 1536\n",
        "\n",
        "For GTX 970 device = 65536 regs per SM\n",
        "\n",
        "Allowed warps = 65536 / 1536 = 42.67\n",
        "\n",
        "For GTX 970 device 98304 regs per SM\n",
        "\n",
        "Active blocks = 98304 / 4096 = 24\n",
        "\n",
        "active warp = 24 * 4 = 96\n",
        "\n",
        "Actice warp does not limit by smem usage.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZUGYyzIIYZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile occupancy.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Your kernel\n",
        "__global__ void occupancy_test(int *results) {\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int x1 = 1, x2 = 2, x3 = 3, x4 = 4, x5 = 5, x6 = 6, x7 = 7, x8 = 8;\n",
        "    results[gid] = x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int blockSize = 128;  // Number of threads per block\n",
        "    const int numBlocks = 80;   // Adjust based on your GPU\n",
        "    const int numThreads = blockSize * numBlocks;\n",
        "\n",
        "    // Allocate memory on device\n",
        "    int *d_results;\n",
        "    cudaMalloc((void**)&d_results, numThreads * sizeof(int));\n",
        "\n",
        "    // Launch the kernel\n",
        "    occupancy_test<<<numBlocks, blockSize>>>(d_results);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Check for errors\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "\n",
        "    // Occupancy calculation\n",
        "    int maxActiveBlocksPerSM = 0;\n",
        "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
        "        &maxActiveBlocksPerSM,\n",
        "        occupancy_test,\n",
        "        blockSize,\n",
        "        0  // dynamic shared memory\n",
        "    );\n",
        "\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, 0);\n",
        "    int numSMs = prop.multiProcessorCount;\n",
        "    int maxThreadsPerSM = prop.maxThreadsPerMultiProcessor;\n",
        "\n",
        "    float occupancy = (maxActiveBlocksPerSM * blockSize) / (float)maxThreadsPerSM;\n",
        "\n",
        "    printf(\"Max active blocks per SM: %d\\n\", maxActiveBlocksPerSM);\n",
        "    printf(\"Threads per SM: %d\\n\", maxActiveBlocksPerSM * blockSize);\n",
        "    printf(\"Max threads per SM (hardware limit): %d\\n\", maxThreadsPerSM);\n",
        "    printf(\"Occupancy: %.2f%%\\n\", occupancy * 100);\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(d_results);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADE5B6vuLLU1",
        "outputId": "afb8c973-5164-4f2d-f0d0-f89be7edce39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing occupancy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o occupancy occupancy.cu\n",
        "!./occupancy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4O0XGlfOHBN",
        "outputId": "69e33d4c-3903-43b4-e4c1-96c992692e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01moccupancy.cu(41)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"numSMs\"\u001b[0m was declared but never referenced\n",
            "      int numSMs = prop.multiProcessorCount;\n",
            "          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "CUDA Error: the provided PTX was compiled with an unsupported toolchain.\n",
            "Max active blocks per SM: 0\n",
            "Threads per SM: 0\n",
            "Max threads per SM (hardware limit): 1024\n",
            "Occupancy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if a kernel is not bandwidth-bound or computation-bound, then increasing occupancy will not necessarily increase performance. In fact, making changes just to increase occupany can have other effects such as additional instructions, more register spills to local memory which is an off-chip memory, more divergent branches.\n",
        "\n",
        "Keep the number of threads per bloack a multiple of warp size (32).\n",
        "\n",
        "Keep number of blocks much greater than the number of SMs to expose sufficient parallelism to your device\n"
      ],
      "metadata": {
        "id": "hVDS7nodNwRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profiling with nvprof\n",
        "\n",
        "Profile driven optimization\n",
        "\n",
        "Use profiling information to optimize the performance of a program in iterative manner\n",
        "\n",
        "\n",
        "nvprof profile modes\n",
        "- summary mode\n",
        "- gpu and api trace mode\n",
        "- event metrics summery mode\n",
        "- event, metrics trace mode\n",
        "```\n",
        "nvprof[options]\n",
        "    [application]\n",
        "    [application-arguments]\n",
        "```"
      ],
      "metadata": {
        "id": "DDbqUZ9gIaCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void my_kernel(int *d_array) {\n",
        "    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    d_array[idx] = idx * idx;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1024;\n",
        "    int *d_array;\n",
        "\n",
        "    cudaMalloc(&d_array, N * sizeof(int));\n",
        "\n",
        "    // Launch kernel\n",
        "    my_kernel<<<4, 256>>>(d_array);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaFree(d_array);\n",
        "\n",
        "    std::cout << \"Kernel executed!\" << std::endl;\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "UuuF8v5dN_CS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103cd9d5-50e8-4413-d09c-0c404e54df06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hello.cu -o hello\n",
        "!nvprof ./hello\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnbW20JiM8wd",
        "outputId": "1e6e4753-c3bb-4739-af20-2d1839e37fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==640== NVPROF is profiling process 640, command: ./hello\n",
            "Kernel executed!\n",
            "==640== Profiling application: ./hello\n",
            "==640== Profiling result:\n",
            "No kernels were profiled.\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            "      API calls:   70.23%  112.16ms         1  112.16ms  112.16ms  112.16ms  cudaMalloc\n",
            "                   29.04%  46.367ms         1  46.367ms  46.367ms  46.367ms  cudaLaunchKernel\n",
            "                    0.53%  853.60us         1  853.60us  853.60us  853.60us  cuDeviceGetPCIBusId\n",
            "                    0.11%  171.74us       114  1.5060us     110ns  64.586us  cuDeviceGetAttribute\n",
            "                    0.07%  118.91us         1  118.91us  118.91us  118.91us  cudaFree\n",
            "                    0.01%  11.577us         1  11.577us  11.577us  11.577us  cuDeviceGetName\n",
            "                    0.01%  9.3500us         1  9.3500us  9.3500us  9.3500us  cudaDeviceSynchronize\n",
            "                    0.00%  1.4240us         3     474ns     132ns  1.0370us  cuDeviceGetCount\n",
            "                    0.00%     777ns         2     388ns     177ns     600ns  cuDeviceGet\n",
            "                    0.00%     514ns         1     514ns     514ns     514ns  cuDeviceTotalMem\n",
            "                    0.00%     489ns         1     489ns     489ns     489ns  cuModuleGetLoadingMode\n",
            "                    0.00%     282ns         1     282ns     282ns     282ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-trace ./hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SOrx9o2M_26",
        "outputId": "83046811-b3c8-4088-90f6-ad3883d07054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==847== NVPROF is profiling process 847, command: ./hello\n",
            "Kernel executed!\n",
            "==847== Profiling application: ./hello\n",
            "==847== Profiling result:\n",
            "No kernels were profiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile heavy_kernel.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void vector_mul_and_sum(float *a, float *b, float *result, int N) {\n",
        "    __shared__ float cache[256];\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int cacheIndex = threadIdx.x;\n",
        "\n",
        "    float temp = 0;\n",
        "    while (tid < N) {\n",
        "        temp += a[tid] * b[tid];\n",
        "        tid += blockDim.x * gridDim.x;\n",
        "    }\n",
        "\n",
        "    cache[cacheIndex] = temp;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Parallel reduction within the block\n",
        "    int i = blockDim.x / 2;\n",
        "    while (i != 0) {\n",
        "        if (cacheIndex < i)\n",
        "            cache[cacheIndex] += cache[cacheIndex + i];\n",
        "        __syncthreads();\n",
        "        i /= 2;\n",
        "    }\n",
        "\n",
        "    if (cacheIndex == 0)\n",
        "        atomicAdd(result, cache[0]);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1 << 20; // 1 million elements\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    float *h_a = new float[N];\n",
        "    float *h_b = new float[N];\n",
        "    float h_result = 0;\n",
        "\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_a[i] = 1.0f;\n",
        "        h_b[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    float *d_a, *d_b, *d_result;\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_b, size);\n",
        "    cudaMalloc(&d_result, sizeof(float));\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_result, &h_result, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch with 256 threads per block and 256 blocks\n",
        "    vector_mul_and_sum<<<256, 256>>>(d_a, d_b, d_result, N);\n",
        "\n",
        "    cudaMemcpy(&h_result, d_result, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Dot product result: \" << h_result << std::endl;\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_result);\n",
        "    delete[] h_a;\n",
        "    delete[] h_b;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7scfsuynNcI2",
        "outputId": "6e29891a-a717-4944-a899-b63081a4ad0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing heavy_kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc heavy_kernel.cu -o heavy_kernel\n",
        "!nvprof ./heavy_kernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0nDk2PoOAF3",
        "outputId": "46cfc93a-1ada-4628-fa89-729bedc80e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1542== NVPROF is profiling process 1542, command: ./heavy_kernel\n",
            "Dot product result: 0\n",
            "==1542== Profiling application: ./heavy_kernel\n",
            "==1542== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.87%  1.5298ms         3  509.93us     640ns  772.88us  [CUDA memcpy HtoD]\n",
            "                    0.13%  2.0480us         1  2.0480us  2.0480us  2.0480us  [CUDA memcpy DtoH]\n",
            "      API calls:   88.49%  89.685ms         3  29.895ms  69.576us  89.543ms  cudaMalloc\n",
            "                    7.82%  7.9226ms         1  7.9226ms  7.9226ms  7.9226ms  cudaLaunchKernel\n",
            "                    3.03%  3.0754ms         4  768.86us  26.210us  1.9750ms  cudaMemcpy\n",
            "                    0.51%  514.11us         3  171.37us  112.90us  207.44us  cudaFree\n",
            "                    0.13%  126.78us       114  1.1120us     103ns  51.304us  cuDeviceGetAttribute\n",
            "                    0.01%  11.627us         1  11.627us  11.627us  11.627us  cuDeviceGetName\n",
            "                    0.01%  5.2030us         1  5.2030us  5.2030us  5.2030us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6440us         3     548ns     131ns  1.2760us  cuDeviceGetCount\n",
            "                    0.00%  1.1070us         2     553ns     158ns     949ns  cuDeviceGet\n",
            "                    0.00%     552ns         1     552ns     552ns     552ns  cuModuleGetLoadingMode\n",
            "                    0.00%     456ns         1     456ns     456ns     456ns  cuDeviceTotalMem\n",
            "                    0.00%     198ns         1     198ns     198ns     198ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel reduction as syncrhonization example\n",
        "\n",
        "- cudaDeviceSynchronize : introduce a global synchronize point in host code.\n",
        "\n",
        "- __syncthreads : syncrhnization with in a block\n",
        "\n",
        "Parallel reduction : General problem of performing commutative and associative operation across vector is known as the reduction problem.\n",
        "\n",
        "Sequential reduction:\n",
        "```\n",
        "int sum =0 ;\n",
        "for (int i= 0 ; i < size ; i++){\n",
        "  sum+= array[i]\n",
        "}\n",
        "```\n",
        "Our approach :\n",
        "\n",
        "- Partition the input vector into smaller chunks.\n",
        "- And each chunk will be summed up separately.\n",
        "- add these partial results from each chunk into a final sum.\n",
        "\n",
        "---------------------------------------------------\n",
        "\n",
        "Neighbored pair approach\n",
        "\n",
        "- we are going to calculate sum of the block in iterative manner and in each iteration selected elements are paired with their neighbor from given offset\n",
        "\n",
        "- for the first iteratino we are going to set 1 as the offset and in each iteration, this offset will be multiplied by 2.\n",
        "\n",
        "- and number of threads which are going to do any effective work will be divide by this offset value\n",
        "\n",
        "Code-segment\n",
        "\n",
        "```\n",
        "for (int offset = 1 ; offset < blockdim.x ; offset *=2 )\n",
        "{\n",
        "  if (tid % (2*offset) == 0){\n",
        "    input[tid] += input[tid + offset];\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "After each iteration we use _syncthreads() function"
      ],
      "metadata": {
        "id": "mpvE_ceSPH0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile common.h\n",
        "#ifndef COMMON_H\n",
        "#define COMMON_H\n",
        "\n",
        "enum InitType { INIT_ZERO = 0, INIT_ONE = 1, INIT_RANDOM = 2 };\n",
        "\n",
        "int reduction_cpu(int *input, const int size);\n",
        "void compare_results(int gpu_result, int cpu_result);\n",
        "void initialize(int *input, int size, int init_type);\n",
        "\n",
        "#endif\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk1GUTPOXfyt",
        "outputId": "879fb9aa-6d29-499f-9d1d-83c4d7e7711d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing common.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile common.cpp\n",
        "#include \"common.h\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "int reduction_cpu(int *input, const int size) {\n",
        "    int sum = 0;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        sum += input[i];\n",
        "    }\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "void compare_results(int gpu_result, int cpu_result) {\n",
        "    printf(\"GPU RESULT : %d, CPU RESULT : %d\\n\", gpu_result, cpu_result);\n",
        "    if (gpu_result == cpu_result) {\n",
        "        printf(\"PASS\\n\");\n",
        "    } else {\n",
        "        printf(\"FAIL\\n\");\n",
        "    }\n",
        "}\n",
        "\n",
        "void initialize(int *input, int size, int init_type) {\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        switch (init_type) {\n",
        "            case INIT_ZERO: input[i] = 0; break;\n",
        "            case INIT_ONE: input[i] = 1; break;\n",
        "            case INIT_RANDOM: input[i] = rand() % 100; break;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-gLBp-jXaev",
        "outputId": "cd97f8ab-825b-425c-e620-926650e9c03b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing common.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parallel_reduction.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void reduction_kernel(int *input, int *temp, int size) {\n",
        "    extern __shared__ int s_data[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Load data from global memory to shared memory\n",
        "    s_data[tid] = (gid < size) ? input[gid] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Tree-based reduction\n",
        "    for (int offset = blockDim.x / 2; offset > 0; offset >>= 1) {\n",
        "        if (tid < offset) {\n",
        "            s_data[tid] += s_data[tid + offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        temp[blockIdx.x] = s_data[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main() {\n",
        "    printf(\"=== GPU Parallel Reduction ===\\n\");\n",
        "\n",
        "    int size = 1024;\n",
        "    int block_size = 128;\n",
        "    int byte_size = size * sizeof(int);\n",
        "    int grid_size = (size + block_size - 1) / block_size;\n",
        "\n",
        "    // Host allocations\n",
        "    int *h_input = (int*)malloc(byte_size);\n",
        "    int *h_temp  = (int*)malloc(grid_size * sizeof(int));\n",
        "\n",
        "    // Initialize\n",
        "    initialize(h_input, size, INIT_ONE);\n",
        "    int cpu_result = reduction_cpu(h_input, size);\n",
        "\n",
        "    // Device allocations\n",
        "    int *d_input, *d_temp;\n",
        "    cudaMalloc((void**)&d_input, byte_size);\n",
        "    cudaMalloc((void**)&d_temp, grid_size * sizeof(int));\n",
        "    cudaMemcpy(d_input, h_input, byte_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel with dynamic shared memory\n",
        "    reduction_kernel<<<grid_size, block_size, block_size * sizeof(int)>>>(d_input, d_temp, size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy partial sums to host\n",
        "    cudaMemcpy(h_temp, d_temp, grid_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on CPU\n",
        "    int gpu_result = 0;\n",
        "    for (int i = 0; i < grid_size; ++i) {\n",
        "        gpu_result += h_temp[i];\n",
        "    }\n",
        "\n",
        "    compare_results(gpu_result, cpu_result);\n",
        "\n",
        "    // Cleanup\n",
        "    free(h_input);\n",
        "    free(h_temp);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_temp);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmA-LWEmXGJh",
        "outputId": "6719cfd3-b488-468c-a4ea-d1e5423d9de4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing parallel_reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc parallel_reduction.cu common.cpp -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt7G_a6aaJvi",
        "outputId": "1af5e9ac-56e2-463c-9c18-d019b1f52a4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU Parallel Reduction ===\n",
            "GPU RESULT : 1024, CPU RESULT : 1024\n",
            "PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divergence in reduction algorithm\n",
        "\n",
        "- Force neighboring threads to perform summatino\n",
        "\n",
        "- Interleaved pairs\n",
        "\n",
        "\n",
        "Forced neighboring threads approach :\n",
        "\n",
        "In a reduction operation, this typically refers to a pattern where:\n",
        "\n",
        "Thread i sums the value of thread i + 1 (its neighbor).\n",
        "\n",
        "Then, i proceeds to sum with i + 2, i + 4, etc., in a loop with offsets doubling each time.\n",
        "\n",
        "But this causes wrap divergence.\n",
        "\n",
        "Within a warp of 32 threads:\n",
        "\n",
        "At offset = 1, half the threads (even tids) execute the summation.\n",
        "\n",
        "At offset = 2, one quarter of the threads (tid % 4 == 0) do the work.\n",
        "\n",
        "At offset = 4, only tid % 8 == 0 does.\n",
        "\n",
        "Threads in a warp take different paths due to the if condition.\n",
        "\n",
        "This divergence forces serialization — the warp waits for all possible paths to complete, even those where some threads are idle.\n",
        "\n",
        "Instead of using % conditions that cause divergence, you can write reduction kernels that use shared memory and stride-based access.\n"
      ],
      "metadata": {
        "id": "91wjx5gpeu4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parallel_reduction_wrap_divergence.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void reduction_kernel(int *int_array, int *temp_array, int size) {\n",
        "    extern __shared__ int s_data[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    int *i_data = int_array + blockDim.x * blockIdx.x ; //local data block pointer\n",
        "\n",
        "    // Load data from global memory to shared memory\n",
        "    s_data[tid] = (gid < size) ? int_array[gid] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Tree-based reduction\n",
        "    for (int offset = 1; offset <= blockDim.x / 2; offset *= 2) {\n",
        "        int index = 2 * offset * tid;\n",
        "        if (index < blockDim.x) {\n",
        "            s_data[index] += s_data[index + offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        temp_array[blockIdx.x] = s_data[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main() {\n",
        "    printf(\"=== GPU Parallel Reduction ===\\n\");\n",
        "\n",
        "    int size = 1024;\n",
        "    int block_size = 128;\n",
        "    int byte_size = size * sizeof(int);\n",
        "    int grid_size = (size + block_size - 1) / block_size;\n",
        "\n",
        "    // Host allocations\n",
        "    int *h_input = (int*)malloc(byte_size);\n",
        "    int *h_temp  = (int*)malloc(grid_size * sizeof(int));\n",
        "\n",
        "    // Initialize\n",
        "    initialize(h_input, size, INIT_ONE);\n",
        "    int cpu_result = reduction_cpu(h_input, size);\n",
        "\n",
        "    // Device allocations\n",
        "    int *d_input, *d_temp;\n",
        "    cudaMalloc((void**)&d_input, byte_size);\n",
        "    cudaMalloc((void**)&d_temp, grid_size * sizeof(int));\n",
        "    cudaMemcpy(d_input, h_input, byte_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel with dynamic shared memory\n",
        "    reduction_kernel<<<grid_size, block_size, block_size * sizeof(int)>>>(d_input, d_temp, size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy partial sums to host\n",
        "    cudaMemcpy(h_temp, d_temp, grid_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on CPU\n",
        "    int gpu_result = 0;\n",
        "    for (int i = 0; i < grid_size; ++i) {\n",
        "        gpu_result += h_temp[i];\n",
        "    }\n",
        "\n",
        "    compare_results(gpu_result, cpu_result);\n",
        "\n",
        "    // Cleanup\n",
        "    free(h_input);\n",
        "    free(h_temp);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_temp);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzmHPbq5cM4H",
        "outputId": "9a4eea7a-c9f8-44b0-b927-09973c56c27d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing parallel_reduction_wrap_divergence.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc parallel_reduction_wrap_divergence.cu common.cpp -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it8rJyqOgluR",
        "outputId": "a50c1dc5-7eff-46df-9424-acc11be3f150"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mparallel_reduction_wrap_divergence.cu(12)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"i_data\"\u001b[0m was declared but never referenced\n",
            "      int *i_data = int_array + blockDim.x * blockIdx.x ;\n",
            "           ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "=== GPU Parallel Reduction ===\n",
            "GPU RESULT : 1024, CPU RESULT : 1024\n",
            "PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interleaved pair :"
      ],
      "metadata": {
        "id": "r34HHS6lhLG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parallel_reduction_wrap_divergence_interleaved.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void reduction_kernel(int *int_array, int *temp_array, int size) {\n",
        "    extern __shared__ int s_data[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    int *i_data = int_array + blockDim.x * blockIdx.x ; //local data block pointer\n",
        "\n",
        "    // Load data from global memory to shared memory\n",
        "    s_data[tid] = (gid < size) ? int_array[gid] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Tree-based reduction\n",
        "    for (int offset = blockDim.x / 2; offset > 0 ; offset >>=1) {\n",
        "\n",
        "        if (tid < offset) {\n",
        "            s_data[tid] += s_data[tid + offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        temp_array[blockIdx.x] = s_data[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main() {\n",
        "    printf(\"=== GPU Parallel Reduction ===\\n\");\n",
        "\n",
        "    int size = 1024;\n",
        "    int block_size = 128;\n",
        "    int byte_size = size * sizeof(int);\n",
        "    int grid_size = (size + block_size - 1) / block_size;\n",
        "\n",
        "    // Host allocations\n",
        "    int *h_input = (int*)malloc(byte_size);\n",
        "    int *h_temp  = (int*)malloc(grid_size * sizeof(int));\n",
        "\n",
        "    // Initialize\n",
        "    initialize(h_input, size, INIT_ONE);\n",
        "    int cpu_result = reduction_cpu(h_input, size);\n",
        "\n",
        "    // Device allocations\n",
        "    int *d_input, *d_temp;\n",
        "    cudaMalloc((void**)&d_input, byte_size);\n",
        "    cudaMalloc((void**)&d_temp, grid_size * sizeof(int));\n",
        "    cudaMemcpy(d_input, h_input, byte_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel with dynamic shared memory\n",
        "    reduction_kernel<<<grid_size, block_size, block_size * sizeof(int)>>>(d_input, d_temp, size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy partial sums to host\n",
        "    cudaMemcpy(h_temp, d_temp, grid_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on CPU\n",
        "    int gpu_result = 0;\n",
        "    for (int i = 0; i < grid_size; ++i) {\n",
        "        gpu_result += h_temp[i];\n",
        "    }\n",
        "\n",
        "    compare_results(gpu_result, cpu_result);\n",
        "\n",
        "    // Cleanup\n",
        "    free(h_input);\n",
        "    free(h_temp);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_temp);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M76CIP5gpPR",
        "outputId": "b1f09829-d2e6-403f-ff95-b98f9b53a829"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting parallel_reduction_wrap_divergence_interleaved.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc parallel_reduction_wrap_divergence_interleaved.cu common.cpp -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiz4JwlHiVF2",
        "outputId": "9e54aefd-1559-4f5a-ec88-ea441022ddc5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mparallel_reduction_wrap_divergence_interleaved.cu(12)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"i_data\"\u001b[0m was declared but never referenced\n",
            "      int *i_data = int_array + blockDim.x * blockIdx.x ;\n",
            "           ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "=== GPU Parallel Reduction ===\n",
            "GPU RESULT : 1024, CPU RESULT : 1024\n",
            "PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop unrolling\n",
        "\n",
        "- In loop unrolling, rather than writing the body of a loop once and using a loop to execute it repeatedly, the body is written in code multiple times.\n",
        "\n",
        "- The number of copies made of the loop body is called loop unrolling factor.\n",
        "\n",
        "Threads blocks unrolling\n",
        "\n",
        "Data blocks\n",
        "```\n",
        "A -> thread block\n",
        "B -> thread block\n",
        "C -> thread block\n",
        "D -> thread block\n",
        "\n",
        "A \\\n",
        "    Threadblock\n",
        "B /\n",
        "C \\\n",
        "    ThreadBlock\n",
        "D /\n",
        "```"
      ],
      "metadata": {
        "id": "_SYknjO5seis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parallel_reduction_wrap_unrolling.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void reduction_unrolling_blocks2(int *int_array, int *temp_array, int size) {\n",
        "    extern __shared__ int s_data[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int BLOCK_OFFSET = blockIdx.x * blockDim.x * 2;\n",
        "    int index = BLOCK_OFFSET + tid;\n",
        "\n",
        "    // Load two elements per thread from global memory\n",
        "    int sum = 0;\n",
        "    if (index < size) {\n",
        "        sum = int_array[index];\n",
        "        if (index + blockDim.x < size) {\n",
        "            sum += int_array[index + blockDim.x];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    s_data[tid] = sum;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Standard reduction in shared memory\n",
        "    for (int offset = blockDim.x / 2; offset > 0; offset >>= 1) {\n",
        "        if (tid < offset) {\n",
        "            s_data[tid] += s_data[tid + offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        temp_array[blockIdx.x] = s_data[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void reduction_unrolling_blocks4(int *int_array, int *temp_array, int size) {\n",
        "    extern __shared__ int s_data[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    int BLOCK_OFFSET = blockIdx.x * blockDim.x * 2;\n",
        "    int index = BLOCK_OFFSET + tid;\n",
        "\n",
        "    // Load data from global memory to shared memory\n",
        "    s_data[tid] = (gid < size) ? int_array[gid] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    if ((index + 3 * blockDim.x) < size){\n",
        "      s_data[index] += s_data[index + blockDim.x];\n",
        "      int a1 = s_data[index];\n",
        "      int a2 = s_data[index + blockDim.x];\n",
        "      int a3 = s_data[index + 2 * blockDim.x];\n",
        "      int a4 = s_data[index + 3 * blockDim.x];\n",
        "\n",
        "      s_data[index] = a1+ a2 + a3+ a4;\n",
        "    }\n",
        "    __syncthreads();\n",
        "    // Tree-based reduction\n",
        "    for (int offset = blockDim.x / 2; offset > 0 ; offset = offset / 2) {\n",
        "\n",
        "        if (tid < offset) {\n",
        "            s_data[tid] += s_data[tid + offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        temp_array[blockIdx.x] = s_data[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"=== GPU Parallel Reduction ===\\n\");\n",
        "\n",
        "    int size = 1024;\n",
        "    int block_size = 128;\n",
        "    int byte_size = size * sizeof(int);\n",
        "    // int grid_size = (size + block_size - 1) / block_size; // for one block kernel\n",
        "    int grid_size = (size + block_size * 2 - 1) / (block_size * 2); // for 2 block kernel\n",
        "\n",
        "    // Host allocations\n",
        "    int *h_input = (int*)malloc(byte_size);\n",
        "    int *h_temp  = (int*)malloc(grid_size * sizeof(int));\n",
        "\n",
        "    // Initialize\n",
        "    initialize(h_input, size, INIT_ONE);\n",
        "    int cpu_result = reduction_cpu(h_input, size);\n",
        "\n",
        "    // Device allocations\n",
        "    int *d_input, *d_temp;\n",
        "    cudaMalloc((void**)&d_input, byte_size);\n",
        "    cudaMalloc((void**)&d_temp, grid_size * sizeof(int));\n",
        "    cudaMemcpy(d_input, h_input, byte_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel with dynamic shared memory\n",
        "    reduction_unrolling_blocks2<<<grid_size, block_size, block_size * sizeof(int)>>>(d_input, d_temp, size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy partial sums to host\n",
        "    cudaMemcpy(h_temp, d_temp, grid_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on CPU\n",
        "    int gpu_result = 0;\n",
        "    for (int i = 0; i < grid_size; ++i) {\n",
        "        gpu_result += h_temp[i];\n",
        "    }\n",
        "\n",
        "    compare_results(gpu_result, cpu_result);\n",
        "\n",
        "    // Cleanup\n",
        "    free(h_input);\n",
        "    free(h_temp);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_temp);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "20kgwWAbiZjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9048202f-3d5f-4790-f052-cb96da46e978"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting parallel_reduction_wrap_unrolling.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc parallel_reduction_wrap_unrolling.cu common.cpp -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tgfjjN2u726",
        "outputId": "b19ea6b7-05d8-4c8d-a5b4-16884bcd24ce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU Parallel Reduction ===\n",
            "GPU RESULT : 1024, CPU RESULT : 1024\n",
            "PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WRAP UNROLLLING\n",
        "\n",
        "Warp unrolling is a GPU optimization technique where instead of each thread processing a single element, each thread processes multiple elements (usually from global memory) within a warp (32 threads). This increases memory throughput and reduces the number of thread launches.\n",
        "\n",
        "Why is it useful :\n",
        "> Memory coalescing: Modern GPUs fetch global memory in 32-, 64-, or 128-byte chunks. Unrolling helps align memory accesses efficiently.\n",
        "\n",
        "> Reduced instruction overhead: Fewer iterations, fewer branches.\n",
        "\n",
        "> Higher occupancy and arithmetic intensity.\n",
        "\n",
        "\n",
        "We'll assume:\n",
        "\n",
        "Array size: 8 elements → int_array = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "blockDim.x = 8\n",
        "\n",
        "gridDim.x = 1 (just one block)\n",
        "\n",
        "So we'll launch 8 threads → tid from 0 to 7\n",
        "\n",
        "For simplicity, assume each thread reads one value from int_array\n",
        "\n",
        "Focus is on how the reduction is done using tree reduction + warp unrolling\n",
        "\n",
        "\n",
        "stopping at 64 prevents warp divergence by:\n",
        "Ensuring the loop only runs when there's no mixed-path execution within a warp\n",
        "\n",
        "Switching to warp-synchronous code (manual unrolling or shfl) for the last 64 elements\n",
        "\n",
        "Avoiding __syncthreads() in the warp phase — which is both unnecessary and unavailable\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cRAD6w0vv6M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parallel_reduction_wrap_unrolling.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void reduction_kernel_wrap_unrolling(int *int_array, int *temp_array, int size) {\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    int *i_data = int_array + blockDim.x * blockIdx.x ; //local data block pointer\n",
        "\n",
        "\n",
        "    // Tree-based reduction\n",
        "    for (int offset = blockDim.x / 2; offset >=64  ; offset  = offset / 2) {\n",
        "\n",
        "        if (tid < offset) {\n",
        "            i_data[tid] += i_data[tid + offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (tid < 32){\n",
        "      volatile int *vsmem = i_data; // happen directly without any caches.\n",
        "      vsmem[tid] += vsmem[tid + 32];\n",
        "      vsmem[tid] += vsmem[tid + 16];\n",
        "      vsmem[tid] += vsmem[tid + 8];\n",
        "      vsmem[tid] += vsmem[tid + 4];\n",
        "      vsmem[tid] += vsmem[tid +2];\n",
        "      vsmem[tid] += vsmem[tid +1];\n",
        "    }\n",
        "    // Write result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        temp_array[blockIdx.x] = i_data[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main() {\n",
        "    printf(\"=== GPU Parallel Reduction ===\\n\");\n",
        "\n",
        "    int size = 1024;\n",
        "    int block_size = 128;\n",
        "    int byte_size = size * sizeof(int);\n",
        "    int grid_size = (size + block_size - 1) / block_size;\n",
        "\n",
        "    // Host allocations\n",
        "    int *h_input = (int*)malloc(byte_size);\n",
        "    int *h_temp  = (int*)malloc(grid_size * sizeof(int));\n",
        "\n",
        "    // Initialize\n",
        "    initialize(h_input, size, INIT_ONE);\n",
        "    int cpu_result = reduction_cpu(h_input, size);\n",
        "\n",
        "    // Device allocations\n",
        "    int *d_input, *d_temp;\n",
        "    cudaMalloc((void**)&d_input, byte_size);\n",
        "    cudaMalloc((void**)&d_temp, grid_size * sizeof(int));\n",
        "    cudaMemcpy(d_input, h_input, byte_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel with dynamic shared memory\n",
        "    reduction_kernel_wrap_unrolling<<<grid_size, block_size, block_size * sizeof(int)>>>(d_input, d_temp, size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy partial sums to host\n",
        "    cudaMemcpy(h_temp, d_temp, grid_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on CPU\n",
        "    int gpu_result = 0;\n",
        "    for (int i = 0; i < grid_size; ++i) {\n",
        "        gpu_result += h_temp[i];\n",
        "    }\n",
        "\n",
        "    compare_results(gpu_result, cpu_result);\n",
        "\n",
        "    // Cleanup\n",
        "    free(h_input);\n",
        "    free(h_temp);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_temp);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2HkcgdavBsZ",
        "outputId": "a147ba89-2b02-4aa3-9f06-cacb25d080b1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting parallel_reduction_wrap_unrolling.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc parallel_reduction_wrap_unrolling.cu common.cpp -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx7e1N_2x_C6",
        "outputId": "6871190d-8afe-46ad-d196-85b7b422e977"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mparallel_reduction_wrap_unrolling.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"gid\"\u001b[0m was declared but never referenced\n",
            "      int gid = blockIdx.x * blockDim.x + tid;\n",
            "          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "=== GPU Parallel Reduction ===\n",
            "GPU RESULT : 1024, CPU RESULT : 1024\n",
            "PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction with complete unrolling\n",
        "\n",
        "Complete unrolling means you eliminate all loops in the final stages of reduction (especially within a warp) and manually write out each step.\n",
        "\n",
        "1st iteration : offset = 512 : first 512 elements will have sum of next 512\n",
        ".....\n",
        "in last iteration till 64 offset we get sum of all elements in the first 64 elements\n"
      ],
      "metadata": {
        "id": "DO8loDjh2fPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction_kernel_complete_unrolling.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void reduction_kernel_complete_unrolling(int *int_array, int *temp_array, int size) {\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    int *i_data = int_array + blockDim.x * blockIdx.x ; //local data block pointer\n",
        "\n",
        "\n",
        "    if(blockDim.x == 1024 && tid < 512)\n",
        "      i_data[tid] += i_data[tid + 512];\n",
        "    __syncthreads();\n",
        "\n",
        "    if(blockDim.x == 512 && tid < 256)\n",
        "      i_data[tid] += i_data[tid +256];\n",
        "    __syncthreads();\n",
        "\n",
        "    if(blockDim.x == 256 && tid < 128)\n",
        "      i_data[tid] += i_data[tid +128];\n",
        "    __syncthreads();\n",
        "\n",
        "    if(blockDim.x == 128 && tid < 64)\n",
        "      i_data[tid] += i_data[tid +64];\n",
        "    __syncthreads();\n",
        "\n",
        "    if (tid < 32){\n",
        "      volatile int *vsmem = i_data; // happen directly without any caches.\n",
        "      vsmem[tid] += vsmem[tid + 32];\n",
        "      vsmem[tid] += vsmem[tid + 16];\n",
        "      vsmem[tid] += vsmem[tid + 8];\n",
        "      vsmem[tid] += vsmem[tid + 4];\n",
        "      vsmem[tid] += vsmem[tid +2];\n",
        "      vsmem[tid] += vsmem[tid +1];\n",
        "    }\n",
        "    // Write result for this block to global memory\n",
        "    if (tid == 0) {\n",
        "        temp_array[blockIdx.x] = i_data[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main() {\n",
        "    printf(\"=== GPU Parallel Reduction ===\\n\");\n",
        "\n",
        "    int size = 1024;\n",
        "    int block_size = 128;\n",
        "    int byte_size = size * sizeof(int);\n",
        "    int grid_size = (size + block_size - 1) / block_size;\n",
        "\n",
        "    // Host allocations\n",
        "    int *h_input = (int*)malloc(byte_size);\n",
        "    int *h_temp  = (int*)malloc(grid_size * sizeof(int));\n",
        "\n",
        "    // Initialize\n",
        "    initialize(h_input, size, INIT_ONE);\n",
        "    int cpu_result = reduction_cpu(h_input, size);\n",
        "\n",
        "    // Device allocations\n",
        "    int *d_input, *d_temp;\n",
        "    cudaMalloc((void**)&d_input, byte_size);\n",
        "    cudaMalloc((void**)&d_temp, grid_size * sizeof(int));\n",
        "    cudaMemcpy(d_input, h_input, byte_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel with dynamic shared memory\n",
        "    reduction_kernel_complete_unrolling<<<grid_size, block_size, block_size * sizeof(int)>>>(d_input, d_temp, size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy partial sums to host\n",
        "    cudaMemcpy(h_temp, d_temp, grid_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on CPU\n",
        "    int gpu_result = 0;\n",
        "    for (int i = 0; i < grid_size; ++i) {\n",
        "        gpu_result += h_temp[i];\n",
        "    }\n",
        "\n",
        "    compare_results(gpu_result, cpu_result);\n",
        "\n",
        "    // Cleanup\n",
        "    free(h_input);\n",
        "    free(h_temp);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_temp);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPMiv2HHySxX",
        "outputId": "972c2506-5bbc-428d-8799-b0fe9930a653"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction_kernel_complete_unrolling.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction_kernel_complete_unrolling.cu common.cpp -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqIXiS_53na9",
        "outputId": "f4f9c065-4cdb-46a3-a801-7ba9db91caf3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mreduction_kernel_complete_unrolling.cu(9)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"gid\"\u001b[0m was declared but never referenced\n",
            "      int gid = blockIdx.x * blockDim.x + tid;\n",
            "          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "=== GPU Parallel Reduction ===\n",
            "GPU RESULT : 1024, CPU RESULT : 1024\n",
            "PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parallel reduction algorithms :\n",
        "- naive neighbored pairs approach\n",
        "- interleaved pair approach\n",
        "- data block unrolling\n",
        "- warp unrolling\n",
        "- completely unrolling\n"
      ],
      "metadata": {
        "id": "5tF3u8kT5bSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic parallelism\n",
        "\n",
        "\n",
        "Dynamic Parallelism allows a CUDA kernel to launch other kernels directly from the GPU (without going back to the CPU).\n",
        "\n",
        "This is useful for problems where:\n",
        "\n",
        "- The amount of work is data-dependent\n",
        "\n",
        "- You need recursion or nested computation\n",
        "\n",
        "- You want to offload CPU kernel launch overhead\n",
        "\n",
        "```\n",
        "__global__ void child_kernel(int *data) {\n",
        "    int idx = threadIdx.x;\n",
        "    data[idx] += 1;\n",
        "}\n",
        "\n",
        "__global__ void parent_kernel(int *data) {\n",
        "    if (threadIdx.x == 0) {\n",
        "        // GPU launching child kernel\n",
        "        child_kernel<<<1, 10>>>(data);\n",
        "        cudaDeviceSynchronize(); // wait for child to finish\n",
        "    }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "- Parent and child grids share the same global and constant memory storage but have distinct local and shared memory\n",
        "\n",
        "- There are two points in the execution of a child grid when its view of memory is fully consistent with the parent thread: at the start of a child grid, and when the child grid completes.\n",
        "\n",
        "- Shared and local memory are private to a thread block or thread, respectively and are not visible or coherent between parent and child.\n",
        "\n",
        "- Parents kernel will be launched from host with one thread block\n",
        "- in each grid, first thread block in the thread block has to launch the child grid.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WX_KzcpB60Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dynamic_parallelism.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <time.h>\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void dynamic_parallelism_check(int size, int depth) {\n",
        "\n",
        "    printf(\"Depth : %d - tid : %d\\n\", depth, threadIdx.x);\n",
        "    if (size == 1){\n",
        "      return;\n",
        "    }\n",
        "    if (threadIdx.x == 0){\n",
        "      dynamic_parallelism_check<<<1, size / 2>>>(size/2, depth +1);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "  dynamic_parallelism_check<<< 1, 16 >>> (16, 0);\n",
        "  cudaDeviceSynchronize();\n",
        "  cudaDeviceReset();\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYD-L9AR55bw",
        "outputId": "51736996-780a-42df-f489-e74086b1ccdf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dynamic_parallelism.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_50 -rdc=true dynamic_parallelism.cu -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omlHOWn89EYp",
        "outputId": "c7708b2c-cfc9-41b9-f12b-8ef7afed95d5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth : 0 - tid : 0\n",
            "Depth : 0 - tid : 1\n",
            "Depth : 0 - tid : 2\n",
            "Depth : 0 - tid : 3\n",
            "Depth : 0 - tid : 4\n",
            "Depth : 0 - tid : 5\n",
            "Depth : 0 - tid : 6\n",
            "Depth : 0 - tid : 7\n",
            "Depth : 0 - tid : 8\n",
            "Depth : 0 - tid : 9\n",
            "Depth : 0 - tid : 10\n",
            "Depth : 0 - tid : 11\n",
            "Depth : 0 - tid : 12\n",
            "Depth : 0 - tid : 13\n",
            "Depth : 0 - tid : 14\n",
            "Depth : 0 - tid : 15\n",
            "Depth : 1 - tid : 0\n",
            "Depth : 1 - tid : 1\n",
            "Depth : 1 - tid : 2\n",
            "Depth : 1 - tid : 3\n",
            "Depth : 1 - tid : 4\n",
            "Depth : 1 - tid : 5\n",
            "Depth : 1 - tid : 6\n",
            "Depth : 1 - tid : 7\n",
            "Depth : 2 - tid : 0\n",
            "Depth : 2 - tid : 1\n",
            "Depth : 2 - tid : 2\n",
            "Depth : 2 - tid : 3\n",
            "Depth : 3 - tid : 0\n",
            "Depth : 3 - tid : 1\n",
            "Depth : 4 - tid : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ub_EL4G9I8h",
        "outputId": "123fac27-ddde-44b7-9fea-d7afc4e69a78"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==19192== NVPROF is profiling process 19192, command: ./reduction\n",
            "Depth : 0 - tid : 0\n",
            "Depth : 0 - tid : 1\n",
            "Depth : 0 - tid : 2\n",
            "Depth : 0 - tid : 3\n",
            "Depth : 0 - tid : 4\n",
            "Depth : 0 - tid : 5\n",
            "Depth : 0 - tid : 6\n",
            "Depth : 0 - tid : 7\n",
            "Depth : 0 - tid : 8\n",
            "Depth : 0 - tid : 9\n",
            "Depth : 0 - tid : 10\n",
            "Depth : 0 - tid : 11\n",
            "Depth : 0 - tid : 12\n",
            "Depth : 0 - tid : 13\n",
            "Depth : 0 - tid : 14\n",
            "Depth : 0 - tid : 15\n",
            "Depth : 1 - tid : 0\n",
            "Depth : 1 - tid : 1\n",
            "Depth : 1 - tid : 2\n",
            "Depth : 1 - tid : 3\n",
            "Depth : 1 - tid : 4\n",
            "Depth : 1 - tid : 5\n",
            "Depth : 1 - tid : 6\n",
            "Depth : 1 - tid : 7\n",
            "Depth : 2 - tid : 0\n",
            "Depth : 2 - tid : 1\n",
            "Depth : 2 - tid : 2\n",
            "Depth : 2 - tid : 3\n",
            "Depth : 3 - tid : 0\n",
            "Depth : 3 - tid : 1\n",
            "Depth : 4 - tid : 0\n",
            "==19192== Profiling application: ./reduction\n",
            "==19192== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  124.83us         1  124.83us  124.83us  124.83us  dynamic_parallelism_check(int, int)\n",
            "      API calls:   73.88%  93.752ms         1  93.752ms  93.752ms  93.752ms  cudaLaunchKernel\n",
            "                   25.39%  32.213ms         1  32.213ms  32.213ms  32.213ms  cudaDeviceReset\n",
            "                    0.54%  679.50us         1  679.50us  679.50us  679.50us  cudaDeviceSynchronize\n",
            "                    0.19%  234.83us       114  2.0590us     103ns  156.09us  cuDeviceGetAttribute\n",
            "                    0.01%  10.745us         1  10.745us  10.745us  10.745us  cuDeviceGetName\n",
            "                    0.00%  4.3940us         1  4.3940us  4.3940us  4.3940us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.1650us         3     388ns     143ns     784ns  cuDeviceGetCount\n",
            "                    0.00%     894ns         2     447ns     166ns     728ns  cuDeviceGet\n",
            "                    0.00%     598ns         1     598ns     598ns     598ns  cuModuleGetLoadingMode\n",
            "                    0.00%     325ns         1     325ns     325ns     325ns  cuDeviceTotalMem\n",
            "                    0.00%     306ns         1     306ns     306ns     306ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile assingment_1.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <time.h>\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void dynamic_parallelism_check(int size, int depth, int parent_id) {\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    printf(\"Parent : %d - Depth : %d - tid : %d - gid : %d blockID: %d\\n\", parent_id ,depth, tid, gid, blockIdx.x);\n",
        "\n",
        "    if (size == 1){\n",
        "      return;\n",
        "    }\n",
        "\n",
        "    if (threadIdx.x == 0 && blockIdx.x == 0){\n",
        "      dynamic_parallelism_check<<<1, size / 2>>>(size/2, depth +1, blockIdx.x);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "  dynamic_parallelism_check<<< 2, 8 >>> (8, 0, -1);\n",
        "  cudaDeviceSynchronize();\n",
        "  cudaDeviceReset();\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRN1qZDVAXJS",
        "outputId": "6232421f-8912-4b68-ac46-4305b54b0ea1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting assingment_1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_50 -rdc=true assingment_1.cu -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTGe8wg2AXF1",
        "outputId": "cff9e662-df00-4f83-a467-e18be64ae1b7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parent : -1 - Depth : 0 - tid : 0 - gid : 8 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 1 - gid : 9 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 2 - gid : 10 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 3 - gid : 11 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 4 - gid : 12 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 5 - gid : 13 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 6 - gid : 14 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 7 - gid : 15 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 1 - gid : 1 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 2 - gid : 2 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 3 - gid : 3 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 4 - gid : 4 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 5 - gid : 5 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 6 - gid : 6 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 7 - gid : 7 blockID: 0\n",
            "Parent : 0 - Depth : 1 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : 0 - Depth : 1 - tid : 1 - gid : 1 blockID: 0\n",
            "Parent : 0 - Depth : 1 - tid : 2 - gid : 2 blockID: 0\n",
            "Parent : 0 - Depth : 1 - tid : 3 - gid : 3 blockID: 0\n",
            "Parent : 0 - Depth : 2 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : 0 - Depth : 2 - tid : 1 - gid : 1 blockID: 0\n",
            "Parent : 0 - Depth : 3 - tid : 0 - gid : 0 blockID: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile assingment_2.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <time.h>\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void dynamic_parallelism_check(int size, int depth, int parent_id) {\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    printf(\"Parent : %d - Depth : %d - tid : %d - gid : %d blockID: %d\\n\", parent_id ,depth, tid, gid, blockIdx.x);\n",
        "\n",
        "    if (size == 1){\n",
        "      return;\n",
        "    }\n",
        "\n",
        "    if (threadIdx.x == 0){\n",
        "      dynamic_parallelism_check<<<1, size / 2>>>(size/2, depth +1, blockIdx.x);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "  dynamic_parallelism_check<<< 2, 8 >>> (8, 0, -1);\n",
        "  cudaDeviceSynchronize();\n",
        "  cudaDeviceReset();\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "789mFReQ9oMZ",
        "outputId": "d58498d6-9d6a-45b3-ab15-84a3f21a97c1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting assingment_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_50 -rdc=true assingment_2.cu -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgZjlnMo_Spj",
        "outputId": "c9caaf06-6427-4f9f-deee-e0ceb03f2220"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parent : -1 - Depth : 0 - tid : 0 - gid : 8 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 1 - gid : 9 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 2 - gid : 10 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 3 - gid : 11 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 4 - gid : 12 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 5 - gid : 13 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 6 - gid : 14 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 7 - gid : 15 blockID: 1\n",
            "Parent : -1 - Depth : 0 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 1 - gid : 1 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 2 - gid : 2 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 3 - gid : 3 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 4 - gid : 4 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 5 - gid : 5 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 6 - gid : 6 blockID: 0\n",
            "Parent : -1 - Depth : 0 - tid : 7 - gid : 7 blockID: 0\n",
            "Parent : 1 - Depth : 1 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : 1 - Depth : 1 - tid : 1 - gid : 1 blockID: 0\n",
            "Parent : 1 - Depth : 1 - tid : 2 - gid : 2 blockID: 0\n",
            "Parent : 1 - Depth : 1 - tid : 3 - gid : 3 blockID: 0\n",
            "Parent : 0 - Depth : 1 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : 0 - Depth : 1 - tid : 1 - gid : 1 blockID: 0\n",
            "Parent : 0 - Depth : 1 - tid : 2 - gid : 2 blockID: 0\n",
            "Parent : 0 - Depth : 1 - tid : 3 - gid : 3 blockID: 0\n",
            "Parent : 0 - Depth : 2 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : 0 - Depth : 2 - tid : 1 - gid : 1 blockID: 0\n",
            "Parent : 0 - Depth : 2 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : 0 - Depth : 2 - tid : 1 - gid : 1 blockID: 0\n",
            "Parent : 0 - Depth : 3 - tid : 0 - gid : 0 blockID: 0\n",
            "Parent : 0 - Depth : 3 - tid : 0 - gid : 0 blockID: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction with dynamic parallelism"
      ],
      "metadata": {
        "id": "C84H6O5iEed6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction_dynamic_programming.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"common.h\"\n",
        "\n",
        "__global__ void gpuRecursiveReduce(int *input, int *output, int size) {\n",
        "    extern __shared__ int sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Load global data into shared memory\n",
        "    sdata[tid] = (gid < size) ? input[gid] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Interleaved reduction\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] += sdata[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Store per-block result\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "\n",
        "    // Recursively reduce further\n",
        "    if (tid == 0 && blockIdx.x == 0) {\n",
        "        int num_blocks = (size + blockDim.x - 1) / blockDim.x;\n",
        "        if (num_blocks > 1) {\n",
        "            int new_blocks = (num_blocks + blockDim.x - 1) / blockDim.x;\n",
        "            gpuRecursiveReduce<<<new_blocks, blockDim.x, blockDim.x * sizeof(int)>>>(output, output, num_blocks);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 1024;\n",
        "    const int block_size = 128;\n",
        "    const int bytes = size * sizeof(int);\n",
        "\n",
        "    // Allocate and initialize host memory\n",
        "    int *h_input = (int *)malloc(bytes);\n",
        "    for (int i = 0; i < size; i++) h_input[i] = 1;\n",
        "    int cpu_result = reduction_cpu(h_input, size);  // should return 1024\n",
        "\n",
        "    // Allocate device memory\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, bytes);\n",
        "    cudaMalloc(&d_output, bytes);  // reuse same buffer for recursive output\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Allow recursive launches\n",
        "    cudaDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, 8);\n",
        "\n",
        "    int grid_size = (size + block_size - 1) / block_size;\n",
        "\n",
        "    // Launch recursive reduction kernel\n",
        "    gpuRecursiveReduce<<<grid_size, block_size, block_size * sizeof(int)>>>(d_input, d_output, size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Read final result from d_output[0]\n",
        "    int gpu_result = 0;\n",
        "    cudaMemcpy(&gpu_result, d_output, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    //printf(\"GPU RESULT : %d, CPU RESULT : %d\\n\", gpu_result, cpu_result);\n",
        "    compare_results(gpu_result, cpu_result);\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    free(h_input);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwC681XTCl7w",
        "outputId": "19deccdb-f64f-48c5-d058-826115e3f86f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction_dynamic_programming.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_50 -rdc=true reduction_dynamic_programming.cu common.cpp -o reduction -gencode arch=compute_75,code=sm_75\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfNJZVq3FcTT",
        "outputId": "882d6e46-d736-4746-9587-717661c90b0b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU RESULT : 1024, CPU RESULT : 1024\n",
            "PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUKmLmM3FkMs",
        "outputId": "df4661f2-085b-476c-efc8-75175fadbc4c"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==42701== NVPROF is profiling process 42701, command: ./reduction\n",
            "GPU RESULT : 1024, CPU RESULT : 1024\n",
            "PASS\n",
            "==42701== Profiling application: ./reduction\n",
            "==42701== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   91.93%  40.480us         1  40.480us  40.480us  40.480us  gpuRecursiveReduce(int*, int*, int)\n",
            "                    4.80%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "                    3.27%  1.4400us         1  1.4400us  1.4400us  1.4400us  [CUDA memcpy HtoD]\n",
            "      API calls:   91.42%  116.53ms         2  58.264ms  6.3060us  116.52ms  cudaMalloc\n",
            "                    8.17%  10.413ms         1  10.413ms  10.413ms  10.413ms  cudaLaunchKernel\n",
            "                    0.18%  231.23us       114  2.0280us     160ns  81.332us  cuDeviceGetAttribute\n",
            "                    0.11%  134.08us         2  67.041us  12.425us  121.66us  cudaFree\n",
            "                    0.05%  66.686us         1  66.686us  66.686us  66.686us  cudaDeviceSynchronize\n",
            "                    0.05%  58.119us         2  29.059us  27.749us  30.370us  cudaMemcpy\n",
            "                    0.01%  17.180us         1  17.180us  17.180us  17.180us  cuDeviceGetName\n",
            "                    0.00%  5.2290us         1  5.2290us  5.2290us  5.2290us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.6720us         1  3.6720us  3.6720us  3.6720us  cudaDeviceSetLimit\n",
            "                    0.00%  2.9290us         3     976ns     304ns  2.0840us  cuDeviceGetCount\n",
            "                    0.00%  2.0180us         2  1.0090us     307ns  1.7110us  cuDeviceGet\n",
            "                    0.00%  1.2510us         1  1.2510us  1.2510us  1.2510us  cuModuleGetLoadingMode\n",
            "                    0.00%     785ns         1     785ns     785ns     785ns  cuDeviceTotalMem\n",
            "                    0.00%     392ns         1     392ns     392ns     392ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- Warp execution\n",
        "- Resouuce partition and latency hiding\n",
        "- Optimizing a cuda program based on cuda execution model\n",
        "- every cuda device is going to have multiple SM and each SM is going to have hundreds of cores\n",
        "- Thread block is schedule to a single SM\n",
        "- warp is basic unit of exeuction in a cuda program\n",
        "- if the sets of threads exeucte different instructino than other part of the warp, then warp divergence occurs.\n",
        "- warp divergence will hinder the perfromance of a cuda\n",
        "- resoucre partitioning in a cuda program\n",
        "- shared memory is a memory which shared by all the threads.\n",
        "- latency of an arithmentic and memory instructions\n",
        "- occupancy is a measurement of number of resident active thread blocks or wraps\n",
        "- syncrhonization between threads with in thread block using __syncthread() function\n",
        "- parallel reduction algorithm\n",
        "-- navieneighbored pairs approach\n",
        "- interlaeved pair apporach\n",
        "- data block unrolling\n",
        "- warp unrolling\n",
        "- complete unrolling\n",
        "\n",
        "- With dynamic parallelism we can launch cuda kernel form another kernel"
      ],
      "metadata": {
        "id": "T6vo_HGESso2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uV5e4uxMSg_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}